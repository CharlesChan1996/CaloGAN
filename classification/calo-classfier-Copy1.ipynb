{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a $\\pi^{+}$ vs. $e^{+}$ model using a deep neural network and a BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was developed and tested with Keras version 2.0.6. If you are using an older version, you might encounter problems with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep  2 19:11:06 2017       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 0000:01:00.0     Off |                  N/A |\n",
      "| 30%   46C    P8     5W / 180W |   7835MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GT 720      Off  | 0000:02:00.0     N/A |                  N/A |\n",
      "| N/A   45C    P0    N/A /  N/A |      0MiB /   979MiB |     N/A      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\n",
      "| 36%   80C    P2   134W / 250W |  11713MiB / 12207MiB |     89%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|    0     28281    C   /home/micky/.venvwrp/keras2tf1/bin/python     7833MiB |\n",
      "|    1                  Not Supported                                         |\n",
      "|    2     11184    C   python                                       11711MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import h5py\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import (Dense, Reshape, Conv2D, LeakyReLU, BatchNormalization,\n",
    "                          LocallyConnected2D, Activation, ZeroPadding2D,\n",
    "                          Dropout, Lambda, Flatten, Input, add)\n",
    "\n",
    "from keras.layers.merge import concatenate, multiply\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from calodata.features import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shortcut function 'concat' will represent np.concatenate across the 0th axis\n",
    "concat = partial(np.concatenate, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_calodata(fpaths):\n",
    "    '''\n",
    "    Returns:\n",
    "    --------\n",
    "        data: a list of 3 numpy arrays, representing the energy deposition in each layer\n",
    "            for a group of showers contained in the file 'fpath'\n",
    "    '''\n",
    "    for fpath in fpaths:\n",
    "        with h5py.File(fpath, 'r') as h5:\n",
    "            try:\n",
    "                data = [concat((data[i], h5['layer_{}'.format(i)][:])) for i in xrange(3)]\n",
    "            except NameError:\n",
    "                data = [h5['layer_{}'.format(i)][:] for i in xrange(3)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which binary classification task to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASS_ONE = 'gamma'\n",
    "# CLASS_TWO = 'eplus'\n",
    "\n",
    "CLASS_ONE = 'piplus'\n",
    "CLASS_TWO = 'eplus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraing data for piplus\n",
      "Extraing data for eplus\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print 'Extraing data for ' + CLASS_ONE\n",
    "# c1 = load_calodata(glob.glob(('../data/classification/{}_extra.hdf5').format(CLASS_ONE)))\n",
    "c1 = load_calodata(glob.glob(('../../fast/{}_angle_position_5deg_xy.h5').format(CLASS_ONE)))\n",
    "print 'Extraing data for ' + CLASS_TWO\n",
    "# c2 = load_calodata(glob.glob(('../data/classification/{}_extra.hdf5').format(CLASS_TWO)))\n",
    "c2 = load_calodata(glob.glob(('../../fast/{}_angle_position_5deg_xy.h5').format(CLASS_TWO)))\n",
    "data = map(concat, zip(c1, c2))\n",
    "\n",
    "labels = np.array([1] * c1[0].shape[0] + [0] * c2[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# print 'Extraing data for ' + CLASS_ONE\n",
    "# c1_old = load_calodata(glob.glob(('../data/classification/{}.hdf5').format(CLASS_ONE)))\n",
    "# print 'Extraing data for ' + CLASS_TWO\n",
    "# c2_old = load_calodata(glob.glob(('../data/classification/{}.hdf5').format(CLASS_TWO)))\n",
    "# data_old = map(concat, zip(c1_old, c2_old))\n",
    "\n",
    "# labels_old = np.array([1] * c1_old[0].shape[0] + [0] * c2_old[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of piplus events = 500000\n",
      "Number of eplus events = 400000\n"
     ]
    }
   ],
   "source": [
    "print 'Number of {} events = {}'.format(CLASS_ONE, c1[0].shape[0])\n",
    "print 'Number of {} events = {}'.format(CLASS_TWO, c2[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pre-written module to calculate the shower shape variables for the showers in our dataset and save them in an object called 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = extract_features(data) # shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_old = extract_features(data_old) # shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for f, fold in zip(features.T, features_old.T):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     bins = np.linspace(f.min(), f.max(), 20)\n",
    "#     _ = plt.hist(f[labels==0], bins=bins, label='f 0', normed=True, histtype='step')\n",
    "#     _ = plt.hist(f[labels==1], bins=bins, label='f 1', normed=True, histtype='step')\n",
    "#     _ = plt.hist(fold[labels_old==0], bins=bins, label='old 0', normed=True, histtype='step')\n",
    "#     _ = plt.hist(fold[labels_old==1], bins=bins, label='old 1', normed=True, histtype='step')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For no good reason, we are shuffling and splitting into train and test set by hand. \n",
    "\n",
    "TO-DO: use sklearn train_test_split\n",
    "\n",
    "TO-DO: save out a final validation set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "np.random.seed(0)\n",
    "ix = np.array(range(len(labels)))\n",
    "np.random.shuffle(ix)\n",
    "\n",
    "# number of examples to train on\n",
    "nb_train = int(0.7 * len(ix))\n",
    "\n",
    "# train test split\n",
    "ix_train = ix[:nb_train]\n",
    "ix_test = ix[nb_train:]\n",
    "\n",
    "features_train = features[ix_train]\n",
    "data_train = [np.expand_dims(d[ix_train], -1) / 1000. for d in data]\n",
    "labels_train = labels[ix_train]\n",
    "\n",
    "features_test = features[ix_test]\n",
    "data_test = [np.expand_dims(d[ix_test], -1) / 1000. for d in data]\n",
    "labels_test = labels[ix_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_train = np.concatenate([d.reshape(d.shape[0], -1) for d in data_train], axis=-1)\n",
    "raveled_test = np.concatenate([d.reshape(d.shape[0], -1) for d in data_test], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAGAN-style discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(image, selu=True, bn=True):\n",
    "    '''\n",
    "    Build LAGAN-style discriminator\n",
    "    '''\n",
    "    x = Conv2D(64, (2, 2), padding='same')(image)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(image)\n",
    "#     x = LocallyConnected2D(8 * 4, (3, 3), padding='valid', strides=(1, 2))(x)\n",
    "    x = Conv2D(8 * 4, (3, 3), padding='valid', strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "#     x = LocallyConnected2D(16 * 4, (2, 2), padding='valid')(x)\n",
    "    x = Conv2D(16 * 4, (2, 2), padding='valid')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "#     x = LocallyConnected2D(32 * 4, (2, 2), padding='valid', strides=(1, 2))(x)\n",
    "    x = Conv2D(32 * 4, (2, 2), padding='valid', strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [d.shape[1:] for d in data_train]\n",
    "\n",
    "x = [Input(shape=sh) for sh in shapes]\n",
    "\n",
    "h = concatenate(map(partial(build_model), x)) \n",
    "\n",
    "h = Dense(256)(h)\n",
    "h = Activation('relu')(h)\n",
    "h = Dropout(0.5)(h)\n",
    "\n",
    "y = Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "image_dnn = Model(x, y)\n",
    "image_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 5, 98, 1)      0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D) (None, 14, 14, 1)     0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D) (None, 14, 8, 1)      0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 3, 48, 32)     320         zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 12, 6, 32)     320         zero_padding2d_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 12, 3, 32)     320         zero_padding2d_7[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 3, 48, 32)     128         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 12, 6, 32)     128         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 12, 3, 32)     128         conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 3, 48, 32)     0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 12, 6, 32)     0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 12, 3, 32)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 3, 48, 32)     0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 12, 6, 32)     0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 12, 3, 32)     0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D) (None, 5, 50, 32)     0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D) (None, 14, 8, 32)     0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D) (None, 14, 5, 32)     0           dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 4, 49, 64)     8256        zero_padding2d_2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 13, 7, 64)     8256        zero_padding2d_5[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 13, 4, 64)     8256        zero_padding2d_8[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 4, 49, 64)     256         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 13, 7, 64)     256         conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 13, 4, 64)     256         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 4, 49, 64)     0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 13, 7, 64)     0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 13, 4, 64)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4, 49, 64)     0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 13, 7, 64)     0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 13, 4, 64)     0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D) (None, 6, 51, 64)     0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D) (None, 15, 9, 64)     0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D) (None, 15, 6, 64)     0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 5, 25, 128)    32896       zero_padding2d_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 14, 4, 128)    32896       zero_padding2d_6[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 14, 3, 128)    32896       zero_padding2d_9[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 5, 25, 128)    512         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 14, 4, 128)    512         conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 14, 3, 128)    512         conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 5, 25, 128)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 14, 4, 128)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 14, 3, 128)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 5, 25, 128)    0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 14, 4, 128)    0           activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 14, 3, 128)    0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 7168)          0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 5376)          0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 28544)         0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           7307520     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 256)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 256)           0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             257         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 7,434,881\n",
      "Trainable params: 7,433,537\n",
      "Non-trainable params: 1,344\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('LAGAN{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441000 samples, validate on 189000 samples\n",
      "Epoch 1/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9956Epoch 00000: val_loss did not improve\n",
      "441000/441000 [==============================] - 147s - loss: 0.0169 - acc: 0.9956 - val_loss: 0.9455 - val_acc: 0.6791\n",
      "Epoch 2/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9957Epoch 00001: val_loss did not improve\n",
      "441000/441000 [==============================] - 147s - loss: 0.0171 - acc: 0.9957 - val_loss: 0.0342 - val_acc: 0.9953\n",
      "Epoch 3/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9957Epoch 00002: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0168 - acc: 0.9957 - val_loss: 0.0187 - val_acc: 0.9963\n",
      "Epoch 4/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9956Epoch 00003: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0201 - val_acc: 0.9966\n",
      "Epoch 5/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9957Epoch 00004: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0169 - acc: 0.9957 - val_loss: 0.2957 - val_acc: 0.8788\n",
      "Epoch 6/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9957Epoch 00005: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0166 - acc: 0.9957 - val_loss: 0.0223 - val_acc: 0.9968\n",
      "Epoch 7/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9957Epoch 00006: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0165 - acc: 0.9957 - val_loss: 0.0206 - val_acc: 0.9970\n",
      "Epoch 8/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9957Epoch 00007: val_loss did not improve\n",
      "441000/441000 [==============================] - 147s - loss: 0.0167 - acc: 0.9957 - val_loss: 0.0157 - val_acc: 0.9966\n",
      "Epoch 9/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9956Epoch 00008: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0336 - val_acc: 0.9934\n",
      "Epoch 10/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9957Epoch 00009: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0163 - acc: 0.9957 - val_loss: 0.0162 - val_acc: 0.9951\n",
      "Epoch 11/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9958Epoch 00010: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0161 - acc: 0.9958 - val_loss: 0.0178 - val_acc: 0.9960\n",
      "Epoch 12/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9957Epoch 00011: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0163 - acc: 0.9957 - val_loss: 0.0184 - val_acc: 0.9943\n",
      "Epoch 13/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9957Epoch 00012: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0164 - acc: 0.9957 - val_loss: 0.0191 - val_acc: 0.9969\n",
      "Epoch 14/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9958Epoch 00013: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0159 - acc: 0.9958 - val_loss: 0.0445 - val_acc: 0.9936\n",
      "Epoch 15/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9958Epoch 00014: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0161 - acc: 0.9958 - val_loss: 0.0180 - val_acc: 0.9949\n",
      "Epoch 16/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9959Epoch 00015: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0155 - val_acc: 0.9973\n",
      "Epoch 17/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9958Epoch 00016: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0159 - acc: 0.9958 - val_loss: 0.0187 - val_acc: 0.9968\n",
      "Epoch 18/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9958Epoch 00017: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0160 - acc: 0.9958 - val_loss: 0.0202 - val_acc: 0.9971\n",
      "Epoch 19/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9959Epoch 00018: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0543 - val_acc: 0.9874\n",
      "Epoch 20/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9958Epoch 00019: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0161 - acc: 0.9958 - val_loss: 0.0138 - val_acc: 0.9968\n",
      "Epoch 21/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958Epoch 00020: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0158 - acc: 0.9958 - val_loss: 0.0133 - val_acc: 0.9962\n",
      "Epoch 22/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00021: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0220 - val_acc: 0.9957\n",
      "Epoch 23/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9959Epoch 00022: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0340 - val_acc: 0.9944\n",
      "Epoch 24/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9959Epoch 00023: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0160 - acc: 0.9959 - val_loss: 0.1783 - val_acc: 0.9385\n",
      "Epoch 25/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958Epoch 00024: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0218 - val_acc: 0.9969\n",
      "Epoch 26/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9959Epoch 00025: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0538 - val_acc: 0.9880\n",
      "Epoch 27/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9960Epoch 00026: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9960 - val_loss: 0.0224 - val_acc: 0.9966\n",
      "Epoch 28/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9959Epoch 00027: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0138 - val_acc: 0.9971\n",
      "Epoch 29/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9959Epoch 00028: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0156 - acc: 0.9959 - val_loss: 0.2600 - val_acc: 0.9197\n",
      "Epoch 30/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9959Epoch 00029: val_loss improved from 0.01324 to 0.01309, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 149s - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0131 - val_acc: 0.9961\n",
      "Epoch 31/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00030: val_loss did not improve\n",
      "441000/441000 [==============================] - 147s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0150 - val_acc: 0.9969\n",
      "Epoch 32/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9960Epoch 00031: val_loss did not improve\n",
      "441000/441000 [==============================] - 149s - loss: 0.0152 - acc: 0.9960 - val_loss: 0.0134 - val_acc: 0.9964\n",
      "Epoch 33/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9959Epoch 00032: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0141 - val_acc: 0.9971\n",
      "Epoch 34/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00033: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0315 - val_acc: 0.9960\n",
      "Epoch 35/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00034: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0155 - val_acc: 0.9954\n",
      "Epoch 36/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9960Epoch 00035: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0155 - acc: 0.9960 - val_loss: 0.0420 - val_acc: 0.9924\n",
      "Epoch 37/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9960Epoch 00036: val_loss improved from 0.01309 to 0.01206, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 149s - loss: 0.0152 - acc: 0.9960 - val_loss: 0.0121 - val_acc: 0.9971\n",
      "Epoch 38/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9960Epoch 00037: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0155 - acc: 0.9960 - val_loss: 0.0164 - val_acc: 0.9966\n",
      "Epoch 39/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00038: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0143 - val_acc: 0.9965\n",
      "Epoch 40/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9960Epoch 00039: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9960 - val_loss: 0.1206 - val_acc: 0.9716\n",
      "Epoch 41/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9959Epoch 00040: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0169 - val_acc: 0.9971\n",
      "Epoch 42/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9959Epoch 00041: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0265 - val_acc: 0.9954\n",
      "Epoch 43/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9961Epoch 00042: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0148 - val_acc: 0.9969\n",
      "Epoch 44/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9961Epoch 00043: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0130 - val_acc: 0.9973\n",
      "Epoch 45/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9960Epoch 00044: val_loss improved from 0.01206 to 0.01133, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 148s - loss: 0.0151 - acc: 0.9960 - val_loss: 0.0113 - val_acc: 0.9969\n",
      "Epoch 46/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9960Epoch 00045: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9960 - val_loss: 0.1263 - val_acc: 0.9593\n",
      "Epoch 47/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9960Epoch 00046: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9960 - val_loss: 0.0616 - val_acc: 0.9897\n",
      "Epoch 48/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00047: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0461 - val_acc: 0.9906\n",
      "Epoch 49/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9960Epoch 00048: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0191 - val_acc: 0.9971\n",
      "Epoch 50/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9961Epoch 00049: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0169 - val_acc: 0.9969\n",
      "Epoch 51/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9961Epoch 00050: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0770 - val_acc: 0.9835\n",
      "Epoch 52/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9960Epoch 00051: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9960 - val_loss: 0.0463 - val_acc: 0.9908\n",
      "Epoch 53/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9960Epoch 00052: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9960 - val_loss: 0.0130 - val_acc: 0.9967\n",
      "Epoch 54/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9960Epoch 00053: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0151 - acc: 0.9960 - val_loss: 0.0157 - val_acc: 0.9970\n",
      "Epoch 55/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9961Epoch 00054: val_loss improved from 0.01133 to 0.01123, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0112 - val_acc: 0.9968\n",
      "Epoch 56/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9960Epoch 00055: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9960 - val_loss: 0.0216 - val_acc: 0.9963\n",
      "Epoch 57/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9961Epoch 00056: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0172 - val_acc: 0.9971\n",
      "Epoch 58/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9960Epoch 00057: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0149 - acc: 0.9960 - val_loss: 0.0142 - val_acc: 0.9959\n",
      "Epoch 59/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9962Epoch 00058: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0148 - acc: 0.9962 - val_loss: 0.0124 - val_acc: 0.9970\n",
      "Epoch 60/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9961Epoch 00059: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0114 - val_acc: 0.9970\n",
      "Epoch 61/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9961Epoch 00060: val_loss improved from 0.01123 to 0.01109, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 148s - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0111 - val_acc: 0.9971\n",
      "Epoch 62/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9961Epoch 00061: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0184 - val_acc: 0.9969\n",
      "Epoch 63/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9960Epoch 00062: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9960 - val_loss: 0.0198 - val_acc: 0.9970\n",
      "Epoch 64/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9962Epoch 00063: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9962 - val_loss: 0.0147 - val_acc: 0.9973\n",
      "Epoch 65/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9961Epoch 00064: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9961 - val_loss: 0.2598 - val_acc: 0.9040\n",
      "Epoch 66/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9961Epoch 00065: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0129 - val_acc: 0.9972\n",
      "Epoch 67/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9961Epoch 00066: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0126 - val_acc: 0.9969\n",
      "Epoch 68/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9961Epoch 00067: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0181 - val_acc: 0.9944\n",
      "Epoch 69/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9959Epoch 00068: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0117 - val_acc: 0.9973\n",
      "Epoch 70/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9960Epoch 00069: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0146 - acc: 0.9960 - val_loss: 0.0160 - val_acc: 0.9960\n",
      "Epoch 71/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9961Epoch 00070: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0139 - val_acc: 0.9973\n",
      "Epoch 72/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9961Epoch 00071: val_loss did not improve\n",
      "441000/441000 [==============================] - 148s - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0129 - val_acc: 0.9971\n",
      "Epoch 00071: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    image_dnn.fit(data_train, labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnn.load_weights('LAGAN{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnn.save_weights('LAGAN{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnn.load_weights('LAGAN{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with h5py.File('LAGAN{}vs{}-split-indices.h5'.format(CLASS_ONE, CLASS_TWO), 'w') as h5:\n",
    "#     h5['train'] = ix_train\n",
    "#     h5['test'] = ix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_image_dnn = image_dnn.predict(data_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a DNN on shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_dnn(x):\n",
    "\n",
    "    h = Dense(256)(x)\n",
    "    h = Dropout(0.2)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "    \n",
    "\n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(32)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "\n",
    "    h = Dense(1)(h)\n",
    "    y = Activation('sigmoid')(h)\n",
    "    \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(features_train.shape[1], ))\n",
    "feature_dnn = Model(x, build_feature_dnn(x))\n",
    "feature_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 215,105\n",
      "Trainable params: 213,057\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('SS{}vs{}-features-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441000 samples, validate on 189000 samples\n",
      "Epoch 1/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9914Epoch 00000: val_loss improved from inf to 0.01230, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 37s - loss: 0.0294 - acc: 0.9914 - val_loss: 0.0123 - val_acc: 0.9965\n",
      "Epoch 2/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9949Epoch 00001: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0142 - val_acc: 0.9960\n",
      "Epoch 3/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9954Epoch 00002: val_loss improved from 0.01230 to 0.01213, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 32s - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0121 - val_acc: 0.9968\n",
      "Epoch 4/100\n",
      "440320/441000 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9959Epoch 00003: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0125 - val_acc: 0.9965\n",
      "Epoch 5/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9959Epoch 00004: val_loss improved from 0.01213 to 0.01065, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 32s - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0107 - val_acc: 0.9970\n",
      "Epoch 6/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9961Epoch 00005: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0136 - val_acc: 0.9965\n",
      "Epoch 7/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9963Epoch 00006: val_loss improved from 0.01065 to 0.01049, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 32s - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0105 - val_acc: 0.9971\n",
      "Epoch 8/100\n",
      "440320/441000 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9963Epoch 00007: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0141 - acc: 0.9963 - val_loss: 0.0108 - val_acc: 0.9971\n",
      "Epoch 9/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9964Epoch 00008: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0136 - acc: 0.9964 - val_loss: 0.0116 - val_acc: 0.9970\n",
      "Epoch 10/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9965Epoch 00009: val_loss improved from 0.01049 to 0.01011, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 32s - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0101 - val_acc: 0.9972\n",
      "Epoch 11/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9965Epoch 00010: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0136 - acc: 0.9965 - val_loss: 0.0106 - val_acc: 0.9971\n",
      "Epoch 12/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9965Epoch 00011: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0136 - acc: 0.9965 - val_loss: 0.0123 - val_acc: 0.9970\n",
      "Epoch 13/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9965Epoch 00012: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0135 - acc: 0.9965 - val_loss: 0.0106 - val_acc: 0.9971\n",
      "Epoch 14/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9966Epoch 00013: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0133 - acc: 0.9966 - val_loss: 0.0104 - val_acc: 0.9970\n",
      "Epoch 15/100\n",
      "440320/441000 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9966Epoch 00014: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0131 - acc: 0.9966 - val_loss: 0.0110 - val_acc: 0.9970\n",
      "Epoch 16/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9966Epoch 00015: val_loss did not improve\n",
      "441000/441000 [==============================] - 32s - loss: 0.0130 - acc: 0.9966 - val_loss: 0.0112 - val_acc: 0.9968\n",
      "Epoch 17/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9965Epoch 00016: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0102 - val_acc: 0.9972\n",
      "Epoch 18/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9968Epoch 00017: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0129 - acc: 0.9968 - val_loss: 0.0102 - val_acc: 0.9973\n",
      "Epoch 19/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9967Epoch 00018: val_loss did not improve\n",
      "441000/441000 [==============================] - 38s - loss: 0.0129 - acc: 0.9967 - val_loss: 0.0103 - val_acc: 0.9971\n",
      "Epoch 20/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9966Epoch 00019: val_loss improved from 0.01011 to 0.00999, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 36s - loss: 0.0128 - acc: 0.9966 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 21/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9967Epoch 00020: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0128 - acc: 0.9967 - val_loss: 0.0180 - val_acc: 0.9951\n",
      "Epoch 22/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9968Epoch 00021: val_loss did not improve\n",
      "441000/441000 [==============================] - 31s - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0101 - val_acc: 0.9973\n",
      "Epoch 23/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968Epoch 00022: val_loss improved from 0.00999 to 0.00994, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 34s - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0099 - val_acc: 0.9972\n",
      "Epoch 24/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9968Epoch 00023: val_loss improved from 0.00994 to 0.00985, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 37s - loss: 0.0126 - acc: 0.9968 - val_loss: 0.0098 - val_acc: 0.9973\n",
      "Epoch 25/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968Epoch 00024: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0104 - val_acc: 0.9971\n",
      "Epoch 26/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9968Epoch 00025: val_loss improved from 0.00985 to 0.00983, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 36s - loss: 0.0124 - acc: 0.9968 - val_loss: 0.0098 - val_acc: 0.9974\n",
      "Epoch 27/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968Epoch 00026: val_loss improved from 0.00983 to 0.00976, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "441000/441000 [==============================] - 36s - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0098 - val_acc: 0.9974\n",
      "Epoch 28/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968Epoch 00027: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0099 - val_acc: 0.9971\n",
      "Epoch 29/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967Epoch 00028: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0125 - acc: 0.9967 - val_loss: 0.0118 - val_acc: 0.9971\n",
      "Epoch 30/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9967Epoch 00029: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0122 - acc: 0.9967 - val_loss: 0.0105 - val_acc: 0.9972\n",
      "Epoch 31/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967Epoch 00030: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0125 - acc: 0.9967 - val_loss: 0.0104 - val_acc: 0.9971\n",
      "Epoch 32/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9968Epoch 00031: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0124 - acc: 0.9968 - val_loss: 0.0105 - val_acc: 0.9973\n",
      "Epoch 33/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9968Epoch 00032: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0120 - acc: 0.9968 - val_loss: 0.0099 - val_acc: 0.9973\n",
      "Epoch 34/100\n",
      "440192/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9968Epoch 00033: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0124 - acc: 0.9968 - val_loss: 0.0109 - val_acc: 0.9970\n",
      "Epoch 35/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9968Epoch 00034: val_loss did not improve\n",
      "441000/441000 [==============================] - 37s - loss: 0.0122 - acc: 0.9968 - val_loss: 0.0104 - val_acc: 0.9973\n",
      "Epoch 36/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9969Epoch 00035: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0121 - acc: 0.9969 - val_loss: 0.0099 - val_acc: 0.9973\n",
      "Epoch 37/100\n",
      "440320/441000 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9969Epoch 00036: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0123 - acc: 0.9969 - val_loss: 0.0105 - val_acc: 0.9971\n",
      "Epoch 38/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968Epoch 00037: val_loss did not improve\n",
      "441000/441000 [==============================] - 36s - loss: 0.0119 - acc: 0.9968 - val_loss: 0.0099 - val_acc: 0.9973\n",
      "Epoch 00037: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    feature_dnn.fit(features_train / features_train.max(axis=0)[np.newaxis, :], labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dnn.load_weights('SS{}vs{}-features-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "feature_dnn.save_weights('SS{}vs{}-features-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dnn.load_weights('SS{}vs{}-features-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269440/270000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_feature_dnn = feature_dnn.predict(features_test / features_test.max(axis=0)[np.newaxis, :], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train a BDT on shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_clf = GradientBoostingClassifier(verbose=2)\n",
    "parameters = {\n",
    "    'n_estimators':[100, 200, 300],\n",
    "    'max_depth':[3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(base_clf, parameters, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_feature_bdt = clf.predict_proba(features_test)[:, 1].ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple DNN on pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_dnn(x):\n",
    "\n",
    "    h = Dense(512)(x)\n",
    "    h = Dropout(0.2)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(1024)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(2048)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "    \n",
    "    h = Dense(1024)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(128)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "\n",
    "    h = Dense(1)(h)\n",
    "    y = Activation('sigmoid')(h)\n",
    "    \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(raveled_train.shape[1], ))\n",
    "raveled_dnn = Model(x, build_simple_dnn(x))\n",
    "raveled_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 504)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               258560    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,131,009\n",
      "Trainable params: 5,121,793\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "raveled_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('PixVec{}vs{}-raveled-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441000 samples, validate on 189000 samples\n",
      "Epoch 1/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9828Epoch 00000: val_loss improved from inf to 0.02403, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 49s - loss: 0.0560 - acc: 0.9828 - val_loss: 0.0240 - val_acc: 0.9936\n",
      "Epoch 2/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9929Epoch 00001: val_loss improved from 0.02403 to 0.01826, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0255 - acc: 0.9929 - val_loss: 0.0183 - val_acc: 0.9949\n",
      "Epoch 3/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9938Epoch 00002: val_loss improved from 0.01826 to 0.01739, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0225 - acc: 0.9938 - val_loss: 0.0174 - val_acc: 0.9950\n",
      "Epoch 4/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9943Epoch 00003: val_loss improved from 0.01739 to 0.01680, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 49s - loss: 0.0208 - acc: 0.9943 - val_loss: 0.0168 - val_acc: 0.9951\n",
      "Epoch 5/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9946Epoch 00004: val_loss improved from 0.01680 to 0.01662, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0200 - acc: 0.9946 - val_loss: 0.0166 - val_acc: 0.9953\n",
      "Epoch 6/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9948Epoch 00005: val_loss improved from 0.01662 to 0.01584, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0187 - acc: 0.9948 - val_loss: 0.0158 - val_acc: 0.9954\n",
      "Epoch 7/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9949Epoch 00006: val_loss did not improve\n",
      "441000/441000 [==============================] - 47s - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0162 - val_acc: 0.9957\n",
      "Epoch 8/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9952Epoch 00007: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0168 - val_acc: 0.9956\n",
      "Epoch 9/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9952Epoch 00008: val_loss improved from 0.01584 to 0.01367, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0172 - acc: 0.9952 - val_loss: 0.0137 - val_acc: 0.9959\n",
      "Epoch 10/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9953Epoch 00009: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 0.9956\n",
      "Epoch 11/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9953Epoch 00010: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0142 - val_acc: 0.9959\n",
      "Epoch 12/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9953Epoch 00011: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9955\n",
      "Epoch 13/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9956Epoch 00012: val_loss did not improve\n",
      "441000/441000 [==============================] - 47s - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0140 - val_acc: 0.9959\n",
      "Epoch 14/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9955Epoch 00013: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0160 - acc: 0.9955 - val_loss: 0.0149 - val_acc: 0.9959\n",
      "Epoch 15/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9956Epoch 00014: val_loss did not improve\n",
      "441000/441000 [==============================] - 49s - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "Epoch 16/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9956Epoch 00015: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "Epoch 17/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9956Epoch 00016: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0154 - acc: 0.9956 - val_loss: 0.0145 - val_acc: 0.9957\n",
      "Epoch 18/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9957Epoch 00017: val_loss improved from 0.01367 to 0.01323, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 48s - loss: 0.0153 - acc: 0.9957 - val_loss: 0.0132 - val_acc: 0.9961\n",
      "Epoch 19/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9958Epoch 00018: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0152 - acc: 0.9958 - val_loss: 0.0133 - val_acc: 0.9962\n",
      "Epoch 20/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9958Epoch 00019: val_loss did not improve\n",
      "441000/441000 [==============================] - 48s - loss: 0.0149 - acc: 0.9958 - val_loss: 0.0135 - val_acc: 0.9960\n",
      "Epoch 21/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9958Epoch 00020: val_loss improved from 0.01323 to 0.01311, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0147 - acc: 0.9958 - val_loss: 0.0131 - val_acc: 0.9961\n",
      "Epoch 22/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9959Epoch 00021: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0134 - val_acc: 0.9963\n",
      "Epoch 23/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9959Epoch 00022: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0134 - val_acc: 0.9961\n",
      "Epoch 24/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9959Epoch 00023: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0133 - val_acc: 0.9963\n",
      "Epoch 25/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9960Epoch 00024: val_loss improved from 0.01311 to 0.01255, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0125 - val_acc: 0.9964\n",
      "Epoch 26/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9960Epoch 00025: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0129 - val_acc: 0.9961\n",
      "Epoch 27/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9961Epoch 00026: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0131 - val_acc: 0.9963\n",
      "Epoch 28/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9961Epoch 00027: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0138 - acc: 0.9961 - val_loss: 0.0137 - val_acc: 0.9961\n",
      "Epoch 29/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9961Epoch 00028: val_loss improved from 0.01255 to 0.01248, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0137 - acc: 0.9961 - val_loss: 0.0125 - val_acc: 0.9964\n",
      "Epoch 30/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9963Epoch 00029: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0136 - acc: 0.9963 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "Epoch 31/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9962Epoch 00030: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "Epoch 32/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9963Epoch 00031: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0134 - acc: 0.9963 - val_loss: 0.0127 - val_acc: 0.9965\n",
      "Epoch 33/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9962Epoch 00032: val_loss improved from 0.01248 to 0.01238, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0132 - acc: 0.9962 - val_loss: 0.0124 - val_acc: 0.9965\n",
      "Epoch 34/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9963Epoch 00033: val_loss improved from 0.01238 to 0.01216, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0122 - val_acc: 0.9965\n",
      "Epoch 35/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9963Epoch 00034: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0127 - val_acc: 0.9964\n",
      "Epoch 36/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9964Epoch 00035: val_loss improved from 0.01216 to 0.01205, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0129 - acc: 0.9964 - val_loss: 0.0121 - val_acc: 0.9966\n",
      "Epoch 37/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9964Epoch 00036: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0127 - acc: 0.9964 - val_loss: 0.0126 - val_acc: 0.9966\n",
      "Epoch 38/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9964Epoch 00037: val_loss improved from 0.01205 to 0.01172, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 46s - loss: 0.0126 - acc: 0.9964 - val_loss: 0.0117 - val_acc: 0.9966\n",
      "Epoch 39/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9965Epoch 00038: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0127 - val_acc: 0.9967\n",
      "Epoch 40/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9965Epoch 00039: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0121 - val_acc: 0.9967\n",
      "Epoch 41/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9965Epoch 00040: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0121 - val_acc: 0.9966\n",
      "Epoch 42/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965Epoch 00041: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0122 - val_acc: 0.9967\n",
      "Epoch 43/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9965Epoch 00042: val_loss did not improve\n",
      "441000/441000 [==============================] - 44s - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0126 - val_acc: 0.9967\n",
      "Epoch 44/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9965Epoch 00043: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0122 - val_acc: 0.9966\n",
      "Epoch 45/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9966Epoch 00044: val_loss improved from 0.01172 to 0.01171, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0122 - acc: 0.9966 - val_loss: 0.0117 - val_acc: 0.9968\n",
      "Epoch 46/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9967Epoch 00045: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0120 - acc: 0.9967 - val_loss: 0.0120 - val_acc: 0.9965\n",
      "Epoch 47/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965Epoch 00046: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0119 - val_acc: 0.9967\n",
      "Epoch 48/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9966Epoch 00047: val_loss improved from 0.01171 to 0.01104, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0120 - acc: 0.9966 - val_loss: 0.0110 - val_acc: 0.9969\n",
      "Epoch 49/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9967Epoch 00048: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0119 - acc: 0.9967 - val_loss: 0.0116 - val_acc: 0.9967\n",
      "Epoch 50/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9967Epoch 00049: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0119 - acc: 0.9967 - val_loss: 0.0134 - val_acc: 0.9964\n",
      "Epoch 51/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9967Epoch 00050: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0129 - val_acc: 0.9965\n",
      "Epoch 52/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9967Epoch 00051: val_loss improved from 0.01104 to 0.01099, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "441000/441000 [==============================] - 45s - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0110 - val_acc: 0.9970\n",
      "Epoch 53/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9967Epoch 00052: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0115 - val_acc: 0.9967\n",
      "Epoch 54/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967Epoch 00053: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0116 - acc: 0.9967 - val_loss: 0.0119 - val_acc: 0.9965\n",
      "Epoch 55/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968Epoch 00054: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0116 - val_acc: 0.9967\n",
      "Epoch 56/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9968Epoch 00055: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0116 - acc: 0.9968 - val_loss: 0.0114 - val_acc: 0.9970\n",
      "Epoch 57/100\n",
      "440704/441000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968Epoch 00056: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0130 - val_acc: 0.9963\n",
      "Epoch 58/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967Epoch 00057: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0116 - acc: 0.9967 - val_loss: 0.0119 - val_acc: 0.9965\n",
      "Epoch 59/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9969Epoch 00058: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441000/441000 [==============================] - 45s - loss: 0.0112 - acc: 0.9969 - val_loss: 0.0123 - val_acc: 0.9964\n",
      "Epoch 60/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9969Epoch 00059: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0111 - acc: 0.9969 - val_loss: 0.0114 - val_acc: 0.9967\n",
      "Epoch 61/100\n",
      "440448/441000 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9968Epoch 00060: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0111 - acc: 0.9968 - val_loss: 0.0115 - val_acc: 0.9969\n",
      "Epoch 62/100\n",
      "440960/441000 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9968Epoch 00061: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0118 - val_acc: 0.9967\n",
      "Epoch 63/100\n",
      "440576/441000 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9970Epoch 00062: val_loss did not improve\n",
      "441000/441000 [==============================] - 45s - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0113 - val_acc: 0.9969\n",
      "Epoch 00062: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raveled_dnn.fit(raveled_train, labels_train, callbacks=callbacks, verbose=True,\n",
    "                          validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_dnn.load_weights('PixVec{}vs{}-raveled-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "raveled_dnn.save_weights('PixVec{}vs{}-raveled-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_dnn.load_weights('PixVec{}vs{}-raveled-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269600/270000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_raveled_dnn = raveled_dnn.predict(raveled_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/micky/keras-contrib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/micky/keras-contrib/keras_contrib/applications\n",
      "/home/micky/CaloGAN/classification\n"
     ]
    }
   ],
   "source": [
    "% cd ~/keras-contrib/keras_contrib/applications/\n",
    "% run densenet.py\n",
    "#from densenet import DenseNet as build_densenet\n",
    "% cd /home/micky/CaloGAN/classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_densenet = DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to support 1 channel images, just comment out the input_shape = _obtain_input_shape part\n",
    "# just set the input_shape when you build the model (risky without checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet.py:507: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_regularizer=<keras.reg..., activation=\"sigmoid\", bias_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "dnet1 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(12, 12, 1), nb_dense_block=1,\n",
    "                      include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 12, 12, 16)    144         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 12, 12, 16)    64          initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 12, 12, 16)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 12, 12, 12)    1728        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 12, 12, 28)    0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 12, 12, 28)    112         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 12, 12, 28)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 12, 12, 12)    3024        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 12, 12, 40)    0           concatenate_1[0][0]              \n",
      "                                                                   conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 12, 12, 40)    160         concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 12, 12, 40)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 12, 12, 12)    4320        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 12, 12, 52)    0           concatenate_2[0][0]              \n",
      "                                                                   conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 12, 12, 52)    208         concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 12, 12, 52)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 12, 12, 12)    5616        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 12, 12, 64)    0           concatenate_3[0][0]              \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 12, 12, 64)    256         concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 12, 12, 64)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 12, 12, 12)    6912        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 12, 12, 76)    0           concatenate_4[0][0]              \n",
      "                                                                   conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 12, 12, 76)    304         concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 12, 12, 76)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 12, 12, 12)    8208        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 12, 12, 88)    0           concatenate_5[0][0]              \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 12, 12, 88)    352         concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 12, 12, 88)    0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 12, 12, 12)    9504        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 12, 12, 100)   0           concatenate_6[0][0]              \n",
      "                                                                   conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 12, 12, 100)   400         concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 12, 12, 100)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 12, 12, 12)    10800       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 12, 12, 112)   0           concatenate_7[0][0]              \n",
      "                                                                   conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 12, 12, 112)   448         concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 12, 12, 112)   0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 12, 12, 12)    12096       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 12, 12, 124)   0           concatenate_8[0][0]              \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 12, 12, 124)   496         concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 12, 12, 124)   0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 12, 12, 12)    13392       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)     (None, 12, 12, 136)   0           concatenate_9[0][0]              \n",
      "                                                                   conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 12, 12, 136)   544         concatenate_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 12, 12, 136)   0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 12, 12, 12)    14688       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)     (None, 12, 12, 148)   0           concatenate_10[0][0]             \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 12, 12, 148)   592         concatenate_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 12, 12, 148)   0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 12, 12, 12)    15984       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)     (None, 12, 12, 160)   0           concatenate_11[0][0]             \n",
      "                                                                   conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 12, 12, 160)   640         concatenate_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 12, 12, 160)   0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 160)           0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             161         global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 111,153\n",
      "Trainable params: 108,865\n",
      "Non-trainable params: 2,288\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_model(dnet, to_file='dnet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='dnet.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=20, monitor='val_loss'),\n",
    "    ModelCheckpoint('DenseNet1{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9937Epoch 00000: val_loss improved from inf to 0.08401, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 127s - loss: 0.0304 - acc: 0.9937 - val_loss: 0.0840 - val_acc: 0.9794\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9938Epoch 00001: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0300 - acc: 0.9938 - val_loss: 0.1758 - val_acc: 0.9652\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9939Epoch 00002: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0294 - acc: 0.9939 - val_loss: 1.3075 - val_acc: 0.7699\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9940Epoch 00003: val_loss improved from 0.08401 to 0.05300, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0288 - acc: 0.9940 - val_loss: 0.0530 - val_acc: 0.9866\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9939Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0288 - acc: 0.9939 - val_loss: 1.3921 - val_acc: 0.7879\n",
      "Epoch 6/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9939Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0286 - acc: 0.9939 - val_loss: 0.1766 - val_acc: 0.9405\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9939Epoch 00006: val_loss improved from 0.05300 to 0.04646, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 125s - loss: 0.0282 - acc: 0.9939 - val_loss: 0.0465 - val_acc: 0.9862\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9939Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0284 - acc: 0.9939 - val_loss: 0.0933 - val_acc: 0.9726\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9941Epoch 00008: val_loss improved from 0.04646 to 0.03845, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0385 - val_acc: 0.9902\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9941Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0537 - val_acc: 0.9852\n",
      "Epoch 11/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9939Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9939 - val_loss: 0.2248 - val_acc: 0.9220\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9941Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0275 - acc: 0.9941 - val_loss: 0.1564 - val_acc: 0.9472\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9940Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0275 - acc: 0.9940 - val_loss: 0.2013 - val_acc: 0.9516\n",
      "Epoch 14/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9941Epoch 00013: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0274 - acc: 0.9941 - val_loss: 0.5369 - val_acc: 0.9107\n",
      "Epoch 15/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9942Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0271 - acc: 0.9942 - val_loss: 0.1172 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9941Epoch 00015: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0270 - acc: 0.9941 - val_loss: 0.3817 - val_acc: 0.9323\n",
      "Epoch 17/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9942Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0272 - acc: 0.9942 - val_loss: 0.8494 - val_acc: 0.7958\n",
      "Epoch 18/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9942Epoch 00017: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0267 - acc: 0.9942 - val_loss: 0.0941 - val_acc: 0.9779\n",
      "Epoch 19/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9941Epoch 00018: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0271 - acc: 0.9941 - val_loss: 0.2104 - val_acc: 0.9416\n",
      "Epoch 20/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9942Epoch 00019: val_loss improved from 0.03845 to 0.03463, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0266 - acc: 0.9942 - val_loss: 0.0346 - val_acc: 0.9918\n",
      "Epoch 21/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9942Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0265 - acc: 0.9942 - val_loss: 0.0352 - val_acc: 0.9906\n",
      "Epoch 22/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9941Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0269 - acc: 0.9941 - val_loss: 0.1083 - val_acc: 0.9786\n",
      "Epoch 23/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9943Epoch 00022: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9943 - val_loss: 0.4564 - val_acc: 0.8818\n",
      "Epoch 24/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0544 - val_acc: 0.9873\n",
      "Epoch 25/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00024: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0265 - acc: 0.9942 - val_loss: 0.3428 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9940Epoch 00025: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0270 - acc: 0.9940 - val_loss: 0.4166 - val_acc: 0.8935\n",
      "Epoch 27/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9942Epoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0262 - acc: 0.9942 - val_loss: 0.0407 - val_acc: 0.9897\n",
      "Epoch 28/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9942Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0261 - acc: 0.9942 - val_loss: 0.2992 - val_acc: 0.8967\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9943Epoch 00028: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0262 - acc: 0.9943 - val_loss: 0.2714 - val_acc: 0.9292\n",
      "Epoch 30/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9943Epoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0260 - acc: 0.9943 - val_loss: 0.1535 - val_acc: 0.9697\n",
      "Epoch 31/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9943Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9943 - val_loss: 0.2747 - val_acc: 0.9565\n",
      "Epoch 32/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00031: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.0946 - val_acc: 0.9695\n",
      "Epoch 33/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.0800 - val_acc: 0.9787\n",
      "Epoch 34/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9943Epoch 00033: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9943 - val_loss: 0.9192 - val_acc: 0.7246\n",
      "Epoch 35/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9921Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0336 - acc: 0.9921 - val_loss: 0.0402 - val_acc: 0.9900\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9940Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0272 - acc: 0.9940 - val_loss: 0.0556 - val_acc: 0.9855\n",
      "Epoch 37/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00036: val_loss improved from 0.03463 to 0.03341, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0334 - val_acc: 0.9928\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9942Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0260 - acc: 0.9942 - val_loss: 0.0418 - val_acc: 0.9896\n",
      "Epoch 39/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9943Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9943 - val_loss: 0.1422 - val_acc: 0.9490\n",
      "Epoch 40/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9944Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0257 - acc: 0.9944 - val_loss: 0.1183 - val_acc: 0.9590\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9943Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9943 - val_loss: 0.0989 - val_acc: 0.9721\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9943Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9943 - val_loss: 0.7811 - val_acc: 0.7953\n",
      "Epoch 43/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9944Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9944 - val_loss: 0.1129 - val_acc: 0.9623\n",
      "Epoch 44/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00043: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0254 - acc: 0.9944 - val_loss: 0.0447 - val_acc: 0.9880\n",
      "Epoch 45/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9944 - val_loss: 1.8572 - val_acc: 0.7730\n",
      "Epoch 46/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9942Epoch 00045: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0261 - acc: 0.9942 - val_loss: 0.1687 - val_acc: 0.9604\n",
      "Epoch 47/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00046: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.6475 - val_acc: 0.8973\n",
      "Epoch 48/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9944Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0253 - acc: 0.9944 - val_loss: 0.1166 - val_acc: 0.9707\n",
      "Epoch 49/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9944Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0253 - acc: 0.9944 - val_loss: 0.0388 - val_acc: 0.9888\n",
      "Epoch 50/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 1.0034 - val_acc: 0.7657\n",
      "Epoch 51/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.0913 - val_acc: 0.9815\n",
      "Epoch 52/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9944Epoch 00051: val_loss improved from 0.03341 to 0.03240, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9944 - val_loss: 0.0324 - val_acc: 0.9927\n",
      "Epoch 53/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0254 - acc: 0.9944 - val_loss: 0.2112 - val_acc: 0.9637\n",
      "Epoch 54/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945Epoch 00053: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.0764 - val_acc: 0.9819\n",
      "Epoch 55/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9942Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0256 - acc: 0.9941 - val_loss: 2.0979 - val_acc: 0.5144\n",
      "Epoch 56/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9935Epoch 00055: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0285 - acc: 0.9935 - val_loss: 0.1472 - val_acc: 0.9562\n",
      "Epoch 57/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9945Epoch 00056: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9945 - val_loss: 0.3062 - val_acc: 0.9372\n",
      "Epoch 58/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00057: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.9211 - val_acc: 0.7731\n",
      "Epoch 59/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9943Epoch 00058: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0251 - acc: 0.9943 - val_loss: 1.0202 - val_acc: 0.7124\n",
      "Epoch 60/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00059: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.4974 - val_acc: 0.8871\n",
      "Epoch 61/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00060: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.4810 - val_acc: 0.9043\n",
      "Epoch 62/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9944Epoch 00061: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0250 - acc: 0.9944 - val_loss: 0.1112 - val_acc: 0.9732\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9942Epoch 00062: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9942 - val_loss: 1.5563 - val_acc: 0.5760\n",
      "Epoch 64/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00063: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.9741 - val_acc: 0.7414\n",
      "Epoch 65/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00064: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.0468 - val_acc: 0.9873\n",
      "Epoch 66/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9944Epoch 00065: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9944 - val_loss: 1.6008 - val_acc: 0.6975\n",
      "Epoch 67/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9944Epoch 00066: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0251 - acc: 0.9944 - val_loss: 2.1769 - val_acc: 0.6182\n",
      "Epoch 68/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9945Epoch 00067: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9945 - val_loss: 0.0353 - val_acc: 0.9914\n",
      "Epoch 69/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945Epoch 00068: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.0630 - val_acc: 0.9853\n",
      "Epoch 70/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9944Epoch 00069: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9944 - val_loss: 0.0496 - val_acc: 0.9871\n",
      "Epoch 71/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9938Epoch 00070: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0273 - acc: 0.9938 - val_loss: 0.2112 - val_acc: 0.9477\n",
      "Epoch 72/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9946Epoch 00071: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9946 - val_loss: 0.7036 - val_acc: 0.8278\n",
      "Epoch 73/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9945Epoch 00072: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9945 - val_loss: 0.0575 - val_acc: 0.9868\n",
      "Epoch 00072: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the middle layer\n",
    "try:\n",
    "    dnet1.fit(data_train[1], labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.load_weights('DenseNet1{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet1.save_weights('DenseNet1{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.load_weights('DenseNet1{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299872/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet = dnet1.predict(data_test[1], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_densenet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:508: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_regularizer=<keras.reg..., activation=\"sigmoid\", bias_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "dnet0 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(3, 96, 1),\n",
    "                       nb_dense_block=1, bottleneck=False) \n",
    "# ugly shapes, but ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 3, 96, 16)     144         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 3, 96, 16)     0           initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 3, 96, 12)     1728        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 3, 96, 28)     0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 3, 96, 28)     0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 3, 96, 12)     3024        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 3, 96, 40)     0           concatenate_1[0][0]              \n",
      "                                                                   conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 3, 96, 40)     0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 3, 96, 12)     4320        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 3, 96, 52)     0           concatenate_2[0][0]              \n",
      "                                                                   conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 3, 96, 52)     0           concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 3, 96, 12)     5616        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 3, 96, 64)     0           concatenate_3[0][0]              \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 3, 96, 64)     0           concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 3, 96, 12)     6912        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 3, 96, 76)     0           concatenate_4[0][0]              \n",
      "                                                                   conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 3, 96, 76)     0           concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 3, 96, 12)     8208        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 3, 96, 88)     0           concatenate_5[0][0]              \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 3, 96, 88)     0           concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 3, 96, 12)     9504        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 3, 96, 100)    0           concatenate_6[0][0]              \n",
      "                                                                   conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 3, 96, 100)    0           concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 3, 96, 12)     10800       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 3, 96, 112)    0           concatenate_7[0][0]              \n",
      "                                                                   conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 3, 96, 112)    0           concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 3, 96, 12)     12096       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 3, 96, 124)    0           concatenate_8[0][0]              \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 3, 96, 124)    0           concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 3, 96, 12)     13392       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)     (None, 3, 96, 136)    0           concatenate_9[0][0]              \n",
      "                                                                   conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 3, 96, 136)    0           concatenate_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 3, 96, 12)     14688       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)     (None, 3, 96, 148)    0           concatenate_10[0][0]             \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 3, 96, 148)    0           concatenate_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 3, 96, 12)     15984       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)     (None, 3, 96, 160)    0           concatenate_11[0][0]             \n",
      "                                                                   conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 3, 96, 160)    640         concatenate_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 3, 96, 160)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 160)           0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             161         global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 107,217\n",
      "Trainable params: 106,897\n",
      "Non-trainable params: 320\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet0.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9781Epoch 00000: val_loss improved from inf to 0.07470, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 103s - loss: 0.1055 - acc: 0.9781 - val_loss: 0.0747 - val_acc: 0.9896\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9894Epoch 00001: val_loss improved from 0.07470 to 0.05956, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0637 - acc: 0.9894 - val_loss: 0.0596 - val_acc: 0.9907\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9911- ETA: 5s - lEpoch 00002: val_loss improved from 0.05956 to 0.05158, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0558 - acc: 0.9911 - val_loss: 0.0516 - val_acc: 0.9924\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9921Epoch 00003: val_loss improved from 0.05158 to 0.04922, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0505 - acc: 0.9921 - val_loss: 0.0492 - val_acc: 0.9928\n",
      "Epoch 5/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9926- ETA: 4s - loss: 0.0482 - aEpoch 00004: val_loss improved from 0.04922 to 0.04667, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0480 - acc: 0.9926 - val_loss: 0.0467 - val_acc: 0.9928\n",
      "Epoch 6/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9929Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0459 - acc: 0.9929 - val_loss: 0.0661 - val_acc: 0.9885\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9930Epoch 00006: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0439 - acc: 0.9930 - val_loss: 0.0799 - val_acc: 0.9856\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9933Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0430 - acc: 0.9933 - val_loss: 0.0507 - val_acc: 0.9911\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9933Epoch 00008: val_loss improved from 0.04667 to 0.04194, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0418 - acc: 0.9933 - val_loss: 0.0419 - val_acc: 0.9931\n",
      "Epoch 10/100\n",
      " 26624/489999 [>.............................] - ETA: 87s - loss: 0.0397 - acc: 0.9939ending early\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the first layer\n",
    "try:\n",
    "    dnet0.fit(data_train[0], labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet0{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet0.load_weights('DenseNet0{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet0.save_weights('DenseNet0{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet0.load_weights('DenseNet0{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299552/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet0 = dnet0.predict(data_test[0], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(12, 6, 1), nb_dense_block=1, bottleneck=False)\n",
    "# it doesn't work with nb_dense_block=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 12, 6, 16)     144         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 12, 6, 16)     0           initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 12, 6, 12)     1728        activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)     (None, 12, 6, 28)     0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 12, 6, 28)     0           concatenate_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 12, 6, 12)     3024        activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 12, 6, 40)     0           concatenate_13[0][0]             \n",
      "                                                                   conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 12, 6, 40)     0           concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 12, 6, 12)     4320        activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 12, 6, 52)     0           concatenate_14[0][0]             \n",
      "                                                                   conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 12, 6, 52)     0           concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 12, 6, 12)     5616        activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 12, 6, 64)     0           concatenate_15[0][0]             \n",
      "                                                                   conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 12, 6, 64)     0           concatenate_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 12, 6, 12)     6912        activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)     (None, 12, 6, 76)     0           concatenate_16[0][0]             \n",
      "                                                                   conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 12, 6, 76)     0           concatenate_17[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 12, 6, 12)     8208        activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)     (None, 12, 6, 88)     0           concatenate_17[0][0]             \n",
      "                                                                   conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 12, 6, 88)     0           concatenate_18[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 12, 6, 12)     9504        activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)     (None, 12, 6, 100)    0           concatenate_18[0][0]             \n",
      "                                                                   conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 12, 6, 100)    0           concatenate_19[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 12, 6, 12)     10800       activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 12, 6, 112)    0           concatenate_19[0][0]             \n",
      "                                                                   conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 12, 6, 112)    0           concatenate_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 12, 6, 12)     12096       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)     (None, 12, 6, 124)    0           concatenate_20[0][0]             \n",
      "                                                                   conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 12, 6, 124)    0           concatenate_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 12, 6, 12)     13392       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 12, 6, 136)    0           concatenate_21[0][0]             \n",
      "                                                                   conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 12, 6, 136)    0           concatenate_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 12, 6, 12)     14688       activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 12, 6, 148)    0           concatenate_22[0][0]             \n",
      "                                                                   conv2d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 12, 6, 148)    0           concatenate_23[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, 12, 6, 12)     15984       activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)     (None, 12, 6, 160)    0           concatenate_23[0][0]             \n",
      "                                                                   conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 12, 6, 160)    640         concatenate_24[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 12, 6, 160)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 160)           0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             161         global_average_pooling2d_2[0][0] \n",
      "====================================================================================================\n",
      "Total params: 107,217\n",
      "Trainable params: 106,897\n",
      "Non-trainable params: 320\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9456Epoch 00000: val_loss improved from inf to 0.15739, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.2031 - acc: 0.9456 - val_loss: 0.1574 - val_acc: 0.9632\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9617Epoch 00001: val_loss improved from 0.15739 to 0.14472, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1602 - acc: 0.9617 - val_loss: 0.1447 - val_acc: 0.9664\n",
      "Epoch 3/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9638Epoch 00002: val_loss improved from 0.14472 to 0.13554, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1494 - acc: 0.9638 - val_loss: 0.1355 - val_acc: 0.9672\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9655Epoch 00003: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1418 - acc: 0.9655 - val_loss: 0.1396 - val_acc: 0.9659\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9661Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1365 - acc: 0.9661 - val_loss: 0.1362 - val_acc: 0.9629\n",
      "Epoch 6/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9663Epoch 00005: val_loss improved from 0.13554 to 0.12710, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1349 - acc: 0.9663 - val_loss: 0.1271 - val_acc: 0.9678\n",
      "Epoch 7/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9664Epoch 00006: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1329 - acc: 0.9664 - val_loss: 0.1329 - val_acc: 0.9675\n",
      "Epoch 8/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9675Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1302 - acc: 0.9675 - val_loss: 0.1355 - val_acc: 0.9616\n",
      "Epoch 9/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9674Epoch 00008: val_loss improved from 0.12710 to 0.12248, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1289 - acc: 0.9674 - val_loss: 0.1225 - val_acc: 0.9696\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9678Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1279 - acc: 0.9678 - val_loss: 0.1287 - val_acc: 0.9686\n",
      "Epoch 11/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9679Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1267 - acc: 0.9679 - val_loss: 0.1320 - val_acc: 0.9630\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9685Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1246 - acc: 0.9685 - val_loss: 0.1247 - val_acc: 0.9696\n",
      "Epoch 13/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9680Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1255 - acc: 0.9680 - val_loss: 0.1514 - val_acc: 0.9580\n",
      "Epoch 14/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9680Epoch 00013: val_loss improved from 0.12248 to 0.12098, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 42s - loss: 0.1261 - acc: 0.9679 - val_loss: 0.1210 - val_acc: 0.9698\n",
      "Epoch 15/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9690Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1218 - acc: 0.9690 - val_loss: 0.1330 - val_acc: 0.9629\n",
      "Epoch 16/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9691Epoch 00015: val_loss improved from 0.12098 to 0.11966, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1216 - acc: 0.9691 - val_loss: 0.1197 - val_acc: 0.9702\n",
      "Epoch 17/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9694Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1201 - acc: 0.9694 - val_loss: 0.1249 - val_acc: 0.9658\n",
      "Epoch 18/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9685Epoch 00017: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1224 - acc: 0.9685 - val_loss: 0.1319 - val_acc: 0.9631\n",
      "Epoch 19/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9699Epoch 00018: val_loss improved from 0.11966 to 0.11797, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1188 - acc: 0.9699 - val_loss: 0.1180 - val_acc: 0.9707\n",
      "Epoch 20/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9705Epoch 00019: val_loss improved from 0.11797 to 0.11297, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1159 - acc: 0.9705 - val_loss: 0.1130 - val_acc: 0.9715\n",
      "Epoch 21/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9703Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1161 - acc: 0.9703 - val_loss: 0.1178 - val_acc: 0.9698\n",
      "Epoch 22/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9705Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1156 - acc: 0.9705 - val_loss: 0.1321 - val_acc: 0.9633\n",
      "Epoch 23/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9713Epoch 00022: val_loss improved from 0.11297 to 0.10589, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1123 - acc: 0.9713 - val_loss: 0.1059 - val_acc: 0.9729\n",
      "Epoch 24/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9715Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1117 - acc: 0.9715 - val_loss: 0.1862 - val_acc: 0.9488\n",
      "Epoch 25/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9717Epoch 00024: val_loss improved from 0.10589 to 0.10536, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1110 - acc: 0.9717 - val_loss: 0.1054 - val_acc: 0.9730\n",
      "Epoch 26/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9722Epoch 00025: val_loss improved from 0.10536 to 0.10183, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1087 - acc: 0.9722 - val_loss: 0.1018 - val_acc: 0.9738\n",
      "Epoch 27/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9722Epoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1084 - acc: 0.9722 - val_loss: 0.1033 - val_acc: 0.9732\n",
      "Epoch 28/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9723Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1089 - acc: 0.9723 - val_loss: 0.1093 - val_acc: 0.9707\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9726Epoch 00028: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1073 - acc: 0.9726 - val_loss: 0.1173 - val_acc: 0.9672\n",
      "Epoch 30/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9727Epoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1067 - acc: 0.9727 - val_loss: 0.1265 - val_acc: 0.9655\n",
      "Epoch 31/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9726Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1071 - acc: 0.9726 - val_loss: 0.1445 - val_acc: 0.9627\n",
      "Epoch 32/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9729Epoch 00031: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1058 - acc: 0.9729 - val_loss: 0.1066 - val_acc: 0.9730\n",
      "Epoch 33/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9729Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1059 - acc: 0.9729 - val_loss: 0.1095 - val_acc: 0.9722\n",
      "Epoch 34/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9731Epoch 00033: val_loss improved from 0.10183 to 0.10149, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 42s - loss: 0.1057 - acc: 0.9731 - val_loss: 0.1015 - val_acc: 0.9731\n",
      "Epoch 35/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9731Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1053 - acc: 0.9731 - val_loss: 0.1044 - val_acc: 0.9725\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9732Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1047 - acc: 0.9732 - val_loss: 0.2070 - val_acc: 0.9302\n",
      "Epoch 37/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9732Epoch 00036: val_loss improved from 0.10149 to 0.09876, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1048 - acc: 0.9732 - val_loss: 0.0988 - val_acc: 0.9742\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9734Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1045 - acc: 0.9734 - val_loss: 0.1014 - val_acc: 0.9731\n",
      "Epoch 39/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9731Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1049 - acc: 0.9731 - val_loss: 0.1051 - val_acc: 0.9716\n",
      "Epoch 40/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9731Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1052 - acc: 0.9731 - val_loss: 0.1206 - val_acc: 0.9678\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9734Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1045 - acc: 0.9734 - val_loss: 0.1157 - val_acc: 0.9682\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9729Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1051 - acc: 0.9729 - val_loss: 0.1054 - val_acc: 0.9724\n",
      "Epoch 43/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9735Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9735 - val_loss: 0.1460 - val_acc: 0.9603\n",
      "Epoch 44/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9734Epoch 00043: val_loss improved from 0.09876 to 0.09765, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1039 - acc: 0.9734 - val_loss: 0.0977 - val_acc: 0.9746\n",
      "Epoch 45/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9736Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1030 - acc: 0.9736 - val_loss: 0.0988 - val_acc: 0.9749\n",
      "Epoch 46/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9735Epoch 00045: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9735 - val_loss: 0.1022 - val_acc: 0.9731\n",
      "Epoch 47/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9734Epoch 00046: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1038 - acc: 0.9734 - val_loss: 0.0978 - val_acc: 0.9746\n",
      "Epoch 48/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9732Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1044 - acc: 0.9732 - val_loss: 0.1160 - val_acc: 0.9687\n",
      "Epoch 49/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9736Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9736 - val_loss: 0.1053 - val_acc: 0.9737\n",
      "Epoch 50/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9735Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9735 - val_loss: 0.1104 - val_acc: 0.9701\n",
      "Epoch 51/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9734Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9734 - val_loss: 0.1087 - val_acc: 0.9720\n",
      "Epoch 52/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9734Epoch 00051: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1038 - acc: 0.9733 - val_loss: 0.1129 - val_acc: 0.9716\n",
      "Epoch 53/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9734Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9734 - val_loss: 0.1289 - val_acc: 0.9654\n",
      "Epoch 54/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9735Epoch 00053: val_loss did not improve\n",
      "489999/489999 [==============================] - 40s - loss: 0.1035 - acc: 0.9735 - val_loss: 0.1029 - val_acc: 0.9725\n",
      "Epoch 55/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9733Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1031 - acc: 0.9734 - val_loss: 0.1045 - val_acc: 0.9720\n",
      "Epoch 00054: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the last layer\n",
    "try:\n",
    "    dnet2.fit(data_train[2], labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet2{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2.load_weights('DenseNet2{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet2.save_weights('DenseNet2{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet2.load_weights('DenseNet2{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299744/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet2 = dnet2.predict(data_test[2], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge DenseNets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [d.shape[1:] for d in data_train]\n",
    "\n",
    "x = [Input(shape=sh) for sh in shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet_layer0 = build_densenet(weights=None, input_shape=(3, 96, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_layer1 = build_densenet(weights=None, input_shape=(12, 12, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_layer2 = build_densenet(weights=None, input_shape=(12, 6, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_merged = [dnet_layer0, dnet_layer1, dnet_layer2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [f(xi) for f, xi in zip(dnet_merged, x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Dense(1, activation='sigmoid')(\n",
    "        Dense(64, activation='relu')(\n",
    "            Concatenate()(features)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_4 (Model)                  (None, 160)           107056      input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_5 (Model)                  (None, 160)           107056      input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_6 (Model)                  (None, 160)           107056      input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)     (None, 480)           0           model_4[1][0]                    \n",
      "                                                                   model_5[1][0]                    \n",
      "                                                                   model_6[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 64)            30784       concatenate_38[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 1)             65          dense_16[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 352,017\n",
      "Trainable params: 351,057\n",
      "Non-trainable params: 960\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_dnet_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441000 samples, validate on 189000 samples\n",
      "Epoch 1/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9856Epoch 00000: val_loss improved from inf to 0.88103, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 300s - loss: 0.1017 - acc: 0.9856 - val_loss: 0.8810 - val_acc: 0.7202\n",
      "Epoch 2/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9943Epoch 00001: val_loss improved from 0.88103 to 0.83109, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0442 - acc: 0.9943 - val_loss: 0.8311 - val_acc: 0.7367\n",
      "Epoch 3/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9951Epoch 00002: val_loss did not improve\n",
      "441000/441000 [==============================] - 297s - loss: 0.0323 - acc: 0.9951 - val_loss: 3.1593 - val_acc: 0.5551\n",
      "Epoch 4/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9954Epoch 00003: val_loss improved from 0.83109 to 0.12908, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 297s - loss: 0.0278 - acc: 0.9954 - val_loss: 0.1291 - val_acc: 0.9757\n",
      "Epoch 5/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9945Epoch 00004: val_loss improved from 0.12908 to 0.03900, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 297s - loss: 0.0303 - acc: 0.9945 - val_loss: 0.0390 - val_acc: 0.9920\n",
      "Epoch 6/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9950Epoch 00005: val_loss improved from 0.03900 to 0.03882, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 297s - loss: 0.0279 - acc: 0.9950 - val_loss: 0.0388 - val_acc: 0.9916\n",
      "Epoch 7/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9958Epoch 00006: val_loss did not improve\n",
      "441000/441000 [==============================] - 297s - loss: 0.0242 - acc: 0.9958 - val_loss: 0.1574 - val_acc: 0.9458\n",
      "Epoch 8/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9965Epoch 00007: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0199 - acc: 0.9965 - val_loss: 0.5535 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9967Epoch 00008: val_loss improved from 0.03882 to 0.02151, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0185 - acc: 0.9967 - val_loss: 0.0215 - val_acc: 0.9954\n",
      "Epoch 10/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9964Epoch 00009: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0190 - acc: 0.9964 - val_loss: 0.0735 - val_acc: 0.9851\n",
      "Epoch 11/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9968Epoch 00010: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0167 - acc: 0.9968 - val_loss: 0.1818 - val_acc: 0.9278\n",
      "Epoch 12/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9970Epoch 00011: val_loss improved from 0.02151 to 0.01934, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0161 - acc: 0.9970 - val_loss: 0.0193 - val_acc: 0.9960\n",
      "Epoch 13/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9969Epoch 00012: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0157 - acc: 0.9969 - val_loss: 0.0423 - val_acc: 0.9907\n",
      "Epoch 14/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9965Epoch 00013: val_loss improved from 0.01934 to 0.01744, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0178 - acc: 0.9965 - val_loss: 0.0174 - val_acc: 0.9966\n",
      "Epoch 15/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9971Epoch 00014: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0154 - acc: 0.9971 - val_loss: 0.4530 - val_acc: 0.7142\n",
      "Epoch 16/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9971Epoch 00015: val_loss improved from 0.01744 to 0.01569, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0153 - acc: 0.9971 - val_loss: 0.0157 - val_acc: 0.9972\n",
      "Epoch 17/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9973Epoch 00016: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0142 - acc: 0.9973 - val_loss: 0.0303 - val_acc: 0.9955\n",
      "Epoch 18/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9973Epoch 00017: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0138 - acc: 0.9973 - val_loss: 0.0167 - val_acc: 0.9966\n",
      "Epoch 19/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9972Epoch 00018: val_loss did not improve\n",
      "441000/441000 [==============================] - 298s - loss: 0.0148 - acc: 0.9972 - val_loss: 0.0310 - val_acc: 0.9937\n",
      "Epoch 20/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9972Epoch 00019: val_loss did not improve\n",
      "441000/441000 [==============================] - 297s - loss: 0.0142 - acc: 0.9972 - val_loss: 3.2123 - val_acc: 0.5541\n",
      "Epoch 21/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9973Epoch 00020: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0142 - acc: 0.9973 - val_loss: 1.9175 - val_acc: 0.5552\n",
      "Epoch 22/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9974Epoch 00021: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0134 - acc: 0.9974 - val_loss: 0.0228 - val_acc: 0.9949\n",
      "Epoch 23/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9974Epoch 00022: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0131 - acc: 0.9974 - val_loss: 0.0950 - val_acc: 0.9774\n",
      "Epoch 24/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9973Epoch 00023: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0130 - acc: 0.9973 - val_loss: 0.0287 - val_acc: 0.9945\n",
      "Epoch 25/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974Epoch 00024: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0129 - acc: 0.9974 - val_loss: 0.0160 - val_acc: 0.9967\n",
      "Epoch 26/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9973Epoch 00025: val_loss improved from 0.01569 to 0.01292, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 296s - loss: 0.0130 - acc: 0.9973 - val_loss: 0.0129 - val_acc: 0.9976\n",
      "Epoch 27/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974Epoch 00026: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0129 - acc: 0.9974 - val_loss: 0.0134 - val_acc: 0.9972\n",
      "Epoch 28/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974Epoch 00027: val_loss did not improve\n",
      "441000/441000 [==============================] - 294s - loss: 0.0129 - acc: 0.9973 - val_loss: 0.0211 - val_acc: 0.9958\n",
      "Epoch 29/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9974Epoch 00028: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0126 - acc: 0.9974 - val_loss: 0.0147 - val_acc: 0.9968\n",
      "Epoch 30/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9975Epoch 00029: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0124 - acc: 0.9975 - val_loss: 0.0389 - val_acc: 0.9884\n",
      "Epoch 31/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9974Epoch 00030: val_loss did not improve\n",
      "441000/441000 [==============================] - 294s - loss: 0.0126 - acc: 0.9974 - val_loss: 0.0238 - val_acc: 0.9955\n",
      "Epoch 32/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9975Epoch 00031: val_loss did not improve\n",
      "441000/441000 [==============================] - 294s - loss: 0.0127 - acc: 0.9975 - val_loss: 0.0142 - val_acc: 0.9970\n",
      "Epoch 33/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9975Epoch 00032: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0121 - acc: 0.9975 - val_loss: 0.0564 - val_acc: 0.9888\n",
      "Epoch 34/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9974Epoch 00033: val_loss did not improve\n",
      "441000/441000 [==============================] - 295s - loss: 0.0123 - acc: 0.9974 - val_loss: 0.0180 - val_acc: 0.9957\n",
      "Epoch 35/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9975Epoch 00034: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0122 - acc: 0.9975 - val_loss: 0.0147 - val_acc: 0.9969\n",
      "Epoch 36/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9974Epoch 00035: val_loss did not improve\n",
      "441000/441000 [==============================] - 294s - loss: 0.0126 - acc: 0.9974 - val_loss: 0.0133 - val_acc: 0.9973\n",
      "Epoch 37/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9975Epoch 00036: val_loss improved from 0.01292 to 0.01235, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "441000/441000 [==============================] - 292s - loss: 0.0121 - acc: 0.9975 - val_loss: 0.0123 - val_acc: 0.9975\n",
      "Epoch 38/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9975Epoch 00037: val_loss did not improve\n",
      "441000/441000 [==============================] - 292s - loss: 0.0119 - acc: 0.9975 - val_loss: 0.0416 - val_acc: 0.9923\n",
      "Epoch 39/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9975Epoch 00038: val_loss did not improve\n",
      "441000/441000 [==============================] - 296s - loss: 0.0117 - acc: 0.9975 - val_loss: 0.0400 - val_acc: 0.9881\n",
      "Epoch 40/100\n",
      "440832/441000 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9974Epoch 00039: val_loss did not improve\n",
      "441000/441000 [==============================] - 297s - loss: 0.0123 - acc: 0.9974 - val_loss: 0.0262 - val_acc: 0.9947\n",
      "Epoch 41/100\n",
      "100096/441000 [=====>........................] - ETA: 206s - loss: 0.0118 - acc: 0.9976ending early\n"
     ]
    }
   ],
   "source": [
    "# Train a merged Dense Net\n",
    "try:\n",
    "    image_dnet_merged.fit(data_train, labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet_noBN_merged_{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged.load_weights('DenseNet_noBN_merged_{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnet_merged.save_weights('DenseNet_noBN_merged_{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnet_merged.load_weights('DenseNet_noBN_merged_{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270000/270000 [==============================] - 59s    \n"
     ]
    }
   ],
   "source": [
    "yhat_dnet_merged = image_dnet_merged.predict(data_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2f8f4dd0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAJVCAYAAADtOuA8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqT8/YBUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeb94a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "bins = np.linspace(0, 1, 100)\n",
    "# plt.hist(yhat_image_dnn[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, Image DNN', color='red')\n",
    "# plt.hist(yhat_image_dnn[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, Image DNN', color='red',\n",
    "#         linestyle='dashed')\n",
    "plt.hist(yhat_dnet_merged[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, DenseNet', color='purple')\n",
    "plt.hist(yhat_dnet_merged[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, DenseNet', color='purple',\n",
    "        linestyle='dashed')\n",
    "plt.hist(yhat_feature_dnn[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, Feature DNN')\n",
    "plt.hist(yhat_feature_dnn[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, Feature DNN')\n",
    "plt.legend(loc='upper center')\n",
    "plt.yscale('log')\n",
    "# plt.xlim((0.99999, 1))\n",
    "# plt.xscale('log')\n",
    "plt.xlabel(r'$\\mathbb{P}[\\pi^{+}]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_image_dnn, tpr_image_dnn, _ = roc_curve(labels_test, abs(1-yhat_image_dnn), pos_label=0)\n",
    "fpr_dnet, tpr_dnet, _ = roc_curve(labels_test, abs(1-yhat_dnet_merged), pos_label=0)\n",
    "# fpr_dnet0, tpr_dnet0, _ = roc_curve(labels_test, abs(1-yhat_dnet0), pos_label=0)\n",
    "# fpr_dnet2, tpr_dnet2, _ = roc_curve(labels_test, abs(1-yhat_dnet2), pos_label=0)\n",
    "# fpr_dnet_mean, tpr_dnet_mean, _ = roc_curve(labels_test,\n",
    "#                                             abs(1 - ((yhat_dnet2 + yhat_dnet + yhat_dnet0)/3) ),\n",
    "#                                             pos_label=0)\n",
    "\n",
    "# fpr_dnet_merged, tpr_dnet_merged, _ = roc_curve(labels_test, abs(1-yhat_dnet_merged), pos_label=0)\n",
    "fpr_raveled_dnn, tpr_raveled_dnn, _ = roc_curve(labels_test, abs(1-yhat_raveled_dnn), pos_label=0)\n",
    "fpr_feature_dnn, tpr_feature_dnn, _ = roc_curve(labels_test, abs(1-yhat_feature_dnn), pos_label=0)\n",
    "# fpr_feature_bdt, tpr_feature_bdt, _ = roc_curve(labels_test, yhat_feature_bdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('{}-vs-{}-outputs.h5'.format(CLASS_ONE, CLASS_TWO), 'w') as h5:\n",
    "    h5['y'] = labels_test\n",
    "    h5['nn_image'] = yhat_image_dnn\n",
    "    h5['nn_raveled'] = yhat_raveled_dnn\n",
    "    h5['nn_showershapes'] = yhat_feature_dnn\n",
    "    h5['bdt_showershapes'] = yhat_feature_bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slices = np.linspace(0, 1, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpr, _ = np.histogram(yhat[labels_test == 1], bins=slices)\n",
    "fpr, _ = np.histogram(yhat[labels_test == 0], bins=slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpr = np.cumsum(tpr[::-1])[::-1]\n",
    "fpr = np.cumsum(fpr[::-1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size' : 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  after removing the cwd from sys.path.\n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f08340f4350>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAJ+CAYAAACTqpd4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4iIiIiUgQJqEREREZEy+H9abSG73qpIngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08341f6ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_dnet, 1 / fpr_dnet, label='DenseNet',\n",
    "        linewidth=2)\n",
    "plt.plot(tpr_image_dnn, 1 / fpr_image_dnn, label='LAGAN-Style Discriminator',\n",
    "        linewidth=2)\n",
    "plt.plot(tpr_feature_dnn, 1 / fpr_feature_dnn, label='FCN on Shower Shapes',\n",
    "        linewidth=2)\n",
    "plt.plot(tpr_raveled_dnn, 1 / fpr_raveled_dnn, label='FCN on Unraveled Pixels',\n",
    "        linewidth=2)\n",
    "\n",
    "\n",
    "# plt.plot(tpr_dnet2, 1 / fpr_dnet2, label='DenseNet 2')\n",
    "# plt.plot(tpr_dnet, 1 / fpr_dnet, label='DenseNet 1')\n",
    "# plt.plot(tpr_dnet0, 1 / fpr_dnet0, label='DenseNet 0')\n",
    "# plt.plot(tpr_dnet_mean, 1 / fpr_dnet_mean, label='DenseNet Mean')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.96, 1))\n",
    "plt.ylim(0, 3000)\n",
    "plt.xlabel(r'$\\varepsilon_{e^+}$')\n",
    "plt.ylabel(r'1 / $\\varepsilon_{\\pi^+}$')\n",
    "\n",
    "# plt.xlabel('{} Efficiency'.format(CLASS_TWO))\n",
    "# plt.ylabel('{} Background Rejection'.format(CLASS_ONE))\n",
    "plt.legend(fontsize=23, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 60% efficiency:\n",
      "FCN on Shower Shapes --> 1198.936 rejection\n",
      "FCN on Unraveled Pixels --> 1110.12592593 rejection\n",
      "LAGAN-Style Discriminator --> 936.66875 rejection\n",
      "LAGAN-Style Discriminator --> 1722.6091954 rejection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  after removing the cwd from sys.path.\n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/micky/.venvwrp/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "wp = 0.98\n",
    "print 'At 60% efficiency:'\n",
    "print 'FCN on Shower Shapes --> {} rejection'.format(\n",
    "    (1 / fpr_feature_dnn)[np.argmin(abs(tpr_feature_dnn - wp))])\n",
    "print 'FCN on Unraveled Pixels --> {} rejection'.format(\n",
    "    (1 / fpr_raveled_dnn)[np.argmin(abs(tpr_raveled_dnn - wp))])\n",
    "print 'LAGAN-Style Discriminator --> {} rejection'.format(\n",
    "    (1 / fpr_image_dnn)[np.argmin(abs(tpr_image_dnn - wp))])\n",
    "print 'LAGAN-Style Discriminator --> {} rejection'.format(\n",
    "    (1 / fpr_dnet)[np.argmin(abs(tpr_dnet - wp))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551724137923269"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1722.6091954 - 1110.12592593) / 1110.12592593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x8be72dd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJQCAYAAADolpLRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM2sC7tSZmZmZNQF36szMzMyawP8D1q4Clfj7oYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x888dff10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_dnet0, 1 / fpr_dnet0, label='DenseNet 0 no BN')\n",
    "plt.plot(tpr_dnet2, 1 / fpr_dnet2, label='DenseNet 2 no BN')\n",
    "plt.plot(tpr_dnet_merged, 1 / fpr_dnet_merged, label='DenseNet')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.98, 1))\n",
    "plt.xlabel('{} Efficiency'.format(CLASS_TWO))\n",
    "plt.ylabel('{} Background Rejection'.format(CLASS_ONE))\n",
    "plt.legend(fontsize=20, loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_image_dnn, fpr_image_dnn, label='Deep NN on Calorimeter Hits')\n",
    "plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='FCNN on Calorimeter Hits')\n",
    "plt.plot(tpr_feature_dnn, fpr_feature_dnn, label='DNN on Shower Shapes')\n",
    "plt.plot(tpr_feature_bdt, fpr_feature_bdt, label='BDT on Shower Shapes')\n",
    "# plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.95, 1))\n",
    "plt.xlabel('{} Efficiency'.format(CLASS_ONE))\n",
    "plt.ylabel('{} Background Efficienct'.format(CLASS_TWO))\n",
    "plt.legend(fontsize=20, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from matplotlib import gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uix(a):\n",
    "    return np.unique(a, return_index=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eff_interp = np.linspace(0.05, 1, 100)\n",
    "# interp_image_dnn = np.interp(eff_interp, tpr_image_dnn, fpr_image_dnn)\n",
    "# interp_feature_dnn = np.interp(eff_interp, tpr_feature_dnn, fpr_feature_dnn)\n",
    "# interp_raveled_dnn = np.interp(eff_interp, tpr_raveled_dnn, fpr_raveled_dnn)\n",
    "\n",
    "interp_image_dnn = interpolate.interp1d(tpr_image_dnn[uix(tpr_image_dnn)], fpr_image_dnn[uix(tpr_image_dnn)], kind='cubic')(eff_interp)\n",
    "interp_feature_dnn = interpolate.interp1d(tpr_feature_dnn[uix(tpr_feature_dnn)], fpr_feature_dnn[uix(tpr_feature_dnn)], kind='cubic')(eff_interp)\n",
    "# interp_raveled_dnn = interpolate.interp1d(tpr_raveled_dnn[uix(tpr_raveled_dnn)], fpr_raveled_dnn[uix(tpr_raveled_dnn)], kind='cubic')(eff_interp)\n",
    "\n",
    "kern_size = 20\n",
    "kern = [1 / float(kern_size)] * kern_size\n",
    "interp_image_dnn = np.convolve(interp_image_dnn, kern, mode='same')\n",
    "interp_feature_dnn = np.convolve(interp_feature_dnn, kern, mode='same')\n",
    "# interp_raveled_dnn = np.convolve(interp_raveled_dnn, kern, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 2]) \n",
    "ax = plt.subplot(gs[0])\n",
    "plt.plot(tpr_image_dnn, fpr_image_dnn, label='ResNet on Calorimeter Hits')\n",
    "plt.plot(tpr_feature_dnn, fpr_feature_dnn, label='DNN on Shower Shapes')\n",
    "# plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0, 1))\n",
    "plt.xlabel(r'$\\pi^{+}$ efficiency')\n",
    "plt.ylabel(r'$e^{+}$ efficiency')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(gs[1], sharex=ax)\n",
    "plt.plot(eff_interp, interp_feature_dnn / interp_image_dnn, label='ResNet on Calorimeter Hits')\n",
    "plt.plot(eff_interp, interp_feature_dnn / interp_feature_dnn, label='DNN on Shower Shapes')\n",
    "# plt.plot(eff_interp, interp_feature_dnn / interp_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "# plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "# plt.xlim((0.99, 1))\n",
    "# plt.xlabel(r'$\\pi^{+}$ efficiency')\n",
    "plt.ylabel(r'Ratio of $e^{+}$ efficiencies of DNN on Shower Shapes to X')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
