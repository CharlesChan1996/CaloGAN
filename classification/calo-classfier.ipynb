{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a $\\pi^{+}$ vs. $e^{+}$ model using a deep neural network and a BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was developed and tested with Keras version 2.0.6. If you are using an older version, you might encounter problems with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 12 18:11:48 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro K420         On   | 0000:01:00.0     Off |                  N/A |\r\n",
      "| 26%   52C    P8    N/A /  N/A |      2MiB /   979MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    On   | 0000:02:00.0     Off |                  N/A |\r\n",
      "| 23%   41C    P8    18W / 250W |   2492MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN X (Pascal)    On   | 0000:03:00.0     Off |                  N/A |\r\n",
      "| 23%   42C    P8    18W / 250W |      2MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1     20228    C   ...borisyak/.virtualenvs/crayfis/bin/python2   355MiB |\r\n",
      "|    1     20446    C   ...borisyak/.virtualenvs/crayfis/bin/python2   257MiB |\r\n",
      "|    1     25025    C   ...borisyak/.virtualenvs/crayfis/bin/python2   561MiB |\r\n",
      "|    1     25449    C   ...borisyak/.virtualenvs/crayfis/bin/python2   241MiB |\r\n",
      "|    1     32297    C   ...borisyak/.virtualenvs/crayfis/bin/python2   719MiB |\r\n",
      "|    1     32665    C   ...borisyak/.virtualenvs/crayfis/bin/python2   355MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import h5py\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import (Dense, Reshape, Conv2D, LeakyReLU, BatchNormalization,\n",
    "                          LocallyConnected2D, Activation, ZeroPadding2D,\n",
    "                          Dropout, Lambda, Flatten, Input, add)\n",
    "\n",
    "from keras.layers.merge import concatenate, multiply\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from calodata.features import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shortcut function 'concat' will represent np.concatenate across the 0th axis\n",
    "concat = partial(np.concatenate, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_calodata(fpaths):\n",
    "    '''\n",
    "    Returns:\n",
    "    --------\n",
    "        data: a list of 3 numpy arrays, representing the energy deposition in each layer\n",
    "            for a group of showers contained in the file 'fpath'\n",
    "    '''\n",
    "    for fpath in fpaths:\n",
    "        with h5py.File(fpath, 'r') as h5:\n",
    "            try:\n",
    "                data = [concat((data[i], h5['layer_{}'.format(i)][:])) for i in xrange(3)]\n",
    "            except NameError:\n",
    "                data = [h5['layer_{}'.format(i)][:] for i in xrange(3)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which binary classification task to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASS_ONE = 'gamma'\n",
    "# CLASS_TWO = 'eplus'\n",
    "\n",
    "CLASS_ONE = 'piplus'\n",
    "CLASS_TWO = 'eplus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraing data for piplus\n",
      "Extraing data for eplus\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print 'Extraing data for ' + CLASS_ONE\n",
    "c1 = load_calodata(glob.glob(('../data/classification/{}_extra.hdf5').format(CLASS_ONE)))\n",
    "print 'Extraing data for ' + CLASS_TWO\n",
    "c2 = load_calodata(glob.glob(('../data/classification/{}_extra.hdf5').format(CLASS_TWO)))\n",
    "data = map(concat, zip(c1, c2))\n",
    "\n",
    "labels = np.array([1] * c1[0].shape[0] + [0] * c2[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# print 'Extraing data for ' + CLASS_ONE\n",
    "# c1_old = load_calodata(glob.glob(('../data/classification/{}.hdf5').format(CLASS_ONE)))\n",
    "# print 'Extraing data for ' + CLASS_TWO\n",
    "# c2_old = load_calodata(glob.glob(('../data/classification/{}.hdf5').format(CLASS_TWO)))\n",
    "# data_old = map(concat, zip(c1_old, c2_old))\n",
    "\n",
    "# labels_old = np.array([1] * c1_old[0].shape[0] + [0] * c2_old[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of piplus events = 500000\n",
      "Number of eplus events = 500000\n"
     ]
    }
   ],
   "source": [
    "print 'Number of {} events = {}'.format(CLASS_ONE, c1[0].shape[0])\n",
    "print 'Number of {} events = {}'.format(CLASS_TWO, c2[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pre-written module to calculate the shower shape variables for the showers in our dataset and save them in an object called 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = extract_features(data) # shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_old = extract_features(data_old) # shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for f, fold in zip(features.T, features_old.T):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     bins = np.linspace(f.min(), f.max(), 20)\n",
    "#     _ = plt.hist(f[labels==0], bins=bins, label='f 0', normed=True, histtype='step')\n",
    "#     _ = plt.hist(f[labels==1], bins=bins, label='f 1', normed=True, histtype='step')\n",
    "#     _ = plt.hist(fold[labels_old==0], bins=bins, label='old 0', normed=True, histtype='step')\n",
    "#     _ = plt.hist(fold[labels_old==1], bins=bins, label='old 1', normed=True, histtype='step')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For no good reason, we are shuffling and splitting into train and test set by hand. \n",
    "\n",
    "TO-DO: use sklearn train_test_split\n",
    "\n",
    "TO-DO: save out a final validation set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "np.random.seed(0)\n",
    "ix = np.array(range(len(labels)))\n",
    "np.random.shuffle(ix)\n",
    "\n",
    "# number of examples to train on\n",
    "nb_train = int(0.7 * len(ix))\n",
    "\n",
    "# train test split\n",
    "ix_train = ix[:nb_train]\n",
    "ix_test = ix[nb_train:]\n",
    "\n",
    "features_train = features[ix_train]\n",
    "data_train = [np.expand_dims(d[ix_train], -1) / 1000. for d in data]\n",
    "labels_train = labels[ix_train]\n",
    "\n",
    "features_test = features[ix_test]\n",
    "data_test = [np.expand_dims(d[ix_test], -1) / 1000. for d in data]\n",
    "labels_test = labels[ix_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_train = np.concatenate([d.reshape(d.shape[0], -1) for d in data_train], axis=-1)\n",
    "raveled_test = np.concatenate([d.reshape(d.shape[0], -1) for d in data_test], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAGAN-style discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(image, selu=True, bn=True):\n",
    "    '''\n",
    "    Build LAGAN-style discriminator\n",
    "    '''\n",
    "    x = Conv2D(64, (2, 2), padding='same')(image)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(image)\n",
    "    x = LocallyConnected2D(8 * 4, (3, 3), padding='valid', strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "    x = LocallyConnected2D(16 * 4, (2, 2), padding='valid')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "    x = LocallyConnected2D(32 * 4, (2, 2), padding='valid', strides=(1, 2))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [d.shape[1:] for d in data_train]\n",
    "\n",
    "x = [Input(shape=sh) for sh in shapes]\n",
    "\n",
    "h = concatenate(map(partial(build_model), x)) \n",
    "\n",
    "h = Dense(256)(h)\n",
    "h = Activation('relu')(h)\n",
    "h = Dropout(0.5)(h)\n",
    "\n",
    "y = Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "image_dnn = Model(x, y)\n",
    "image_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 5, 98, 1)      0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D) (None, 14, 14, 1)     0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D) (None, 14, 8, 1)      0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_1 (LocallyCo (None, 3, 48, 32)     46080       zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_4 (LocallyCo (None, 12, 6, 32)     23040       zero_padding2d_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_7 (LocallyCo (None, 12, 3, 32)     11520       zero_padding2d_7[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 3, 48, 32)     128         locally_connected2d_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 12, 6, 32)     128         locally_connected2d_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 12, 3, 32)     128         locally_connected2d_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 3, 48, 32)     0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 12, 6, 32)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 12, 3, 32)     0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 3, 48, 32)     0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 12, 6, 32)     0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 12, 3, 32)     0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D) (None, 5, 50, 32)     0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D) (None, 14, 8, 32)     0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D) (None, 14, 5, 32)     0           dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_2 (LocallyCo (None, 4, 49, 64)     1618176     zero_padding2d_2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_5 (LocallyCo (None, 13, 7, 64)     751296      zero_padding2d_5[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_8 (LocallyCo (None, 13, 4, 64)     429312      zero_padding2d_8[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 4, 49, 64)     256         locally_connected2d_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 13, 7, 64)     256         locally_connected2d_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 13, 4, 64)     256         locally_connected2d_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 4, 49, 64)     0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 13, 7, 64)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 13, 4, 64)     0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 4, 49, 64)     0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 13, 7, 64)     0           activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 13, 4, 64)     0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D) (None, 6, 51, 64)     0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D) (None, 15, 9, 64)     0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D) (None, 15, 6, 64)     0           dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_3 (LocallyCo (None, 5, 25, 128)    4112000     zero_padding2d_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_6 (LocallyCo (None, 14, 4, 128)    1842176     zero_padding2d_6[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "locally_connected2d_9 (LocallyCo (None, 14, 3, 128)    1381632     zero_padding2d_9[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 5, 25, 128)    512         locally_connected2d_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 14, 4, 128)    512         locally_connected2d_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 14, 3, 128)    512         locally_connected2d_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 5, 25, 128)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 14, 4, 128)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 14, 3, 128)    0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 5, 25, 128)    0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 14, 4, 128)    0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 14, 3, 128)    0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 7168)          0           dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 5376)          0           dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 28544)         0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           7307520     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 256)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 256)           0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             257         dropout_18[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 17,525,697\n",
      "Trainable params: 17,524,353\n",
      "Non-trainable params: 1,344\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('LAGAN{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9934Epoch 00000: val_loss improved from inf to 0.01347, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 300s - loss: 0.0262 - acc: 0.9934 - val_loss: 0.0135 - val_acc: 0.9961\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9958- ETA: 1s - loss: 0.Epoch 00001: val_loss improved from 0.01347 to 0.01004, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0166 - acc: 0.9958 - val_loss: 0.0100 - val_acc: 0.9971\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9963Epoch 00002: val_loss did not improve\n",
      "489999/489999 [==============================] - 287s - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0140 - val_acc: 0.9966\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966Epoch 00003: val_loss improved from 0.01004 to 0.00796, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0134 - acc: 0.9966 - val_loss: 0.0080 - val_acc: 0.9976\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9968Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0086 - val_acc: 0.9975\n",
      "Epoch 6/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9972- Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0120 - acc: 0.9972 - val_loss: 0.0119 - val_acc: 0.9959\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9973Epoch 00006: val_loss improved from 0.00796 to 0.00705, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0112 - acc: 0.9973 - val_loss: 0.0071 - val_acc: 0.9980\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974Epoch 00007: val_loss improved from 0.00705 to 0.00657, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0105 - acc: 0.9974 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9975Epoch 00008: val_loss improved from 0.00657 to 0.00566, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0104 - acc: 0.9975 - val_loss: 0.0057 - val_acc: 0.9986\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9976Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0060 - val_acc: 0.9983\n",
      "Epoch 11/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9977Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0097 - acc: 0.9977 - val_loss: 0.0062 - val_acc: 0.9988\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9978- ETA: 1s - loss: Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0097 - acc: 0.9978 - val_loss: 0.0062 - val_acc: 0.9979\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9977Epoch 00012: val_loss improved from 0.00566 to 0.00550, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0099 - acc: 0.9977 - val_loss: 0.0055 - val_acc: 0.9983\n",
      "Epoch 14/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9978Epoch 00013: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0090 - acc: 0.9978 - val_loss: 0.0057 - val_acc: 0.9984\n",
      "Epoch 15/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9979Epoch 00014: val_loss improved from 0.00550 to 0.00490, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0090 - acc: 0.9979 - val_loss: 0.0049 - val_acc: 0.9985\n",
      "Epoch 16/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9978Epoch 00015: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0089 - acc: 0.9978 - val_loss: 0.0059 - val_acc: 0.9983\n",
      "Epoch 17/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9979Epoch 00016: val_loss improved from 0.00490 to 0.00480, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0089 - acc: 0.9979 - val_loss: 0.0048 - val_acc: 0.9987\n",
      "Epoch 18/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9979Epoch 00017: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0050 - val_acc: 0.9987\n",
      "Epoch 19/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9979- ETA: 1s -Epoch 00018: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0088 - acc: 0.9979 - val_loss: 0.0059 - val_acc: 0.9985\n",
      "Epoch 20/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9981Epoch 00019: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0050 - val_acc: 0.9985\n",
      "Epoch 21/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9980Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0086 - acc: 0.9980 - val_loss: 0.0053 - val_acc: 0.9983\n",
      "Epoch 22/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9980Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0082 - acc: 0.9980 - val_loss: 0.0052 - val_acc: 0.9988\n",
      "Epoch 23/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980Epoch 00022: val_loss improved from 0.00480 to 0.00463, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 289s - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0046 - val_acc: 0.9989\n",
      "Epoch 24/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9981Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0063 - val_acc: 0.9978\n",
      "Epoch 25/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9982Epoch 00024: val_loss did not improve\n",
      "489999/489999 [==============================] - 287s - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0061 - val_acc: 0.9985\n",
      "Epoch 26/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980Epoch 00025: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0054 - val_acc: 0.9988\n",
      "Epoch 27/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982- ETA: 0s - loss: 0.0079 - acc: 0.9 - ETA: 0s - loss: 0.0079 - acc: 0.9 - ETA: 0s - loss: 0.0079 - accEpoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0058 - val_acc: 0.9984\n",
      "Epoch 28/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9982Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0059 - val_acc: 0.9989\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982Epoch 00028: val_loss improved from 0.00463 to 0.00441, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 288s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0044 - val_acc: 0.9988\n",
      "Epoch 30/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9982- ETA: 1s - lEpoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0049 - val_acc: 0.9989\n",
      "Epoch 31/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982- ETA: 9 -Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0046 - val_acc: 0.9988\n",
      "Epoch 32/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9982Epoch 00031: val_loss improved from 0.00441 to 0.00428, saving model to LAGANpiplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 289s - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0043 - val_acc: 0.9988\n",
      "Epoch 33/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9981- ETA: 0s - loss: 0.0078 - acc:Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0077 - acc: 0.9981 - val_loss: 0.0045 - val_acc: 0.9989\n",
      "Epoch 34/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9982Epoch 00033: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0048 - val_acc: 0.9989\n",
      "Epoch 35/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9982Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0044 - val_acc: 0.9988\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9983Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0054 - val_acc: 0.9990\n",
      "Epoch 37/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9983Epoch 00036: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0051 - val_acc: 0.9986\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9982Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0056 - val_acc: 0.9990\n",
      "Epoch 39/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9983- ETA: 0s - loss: 0.0076Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0047 - val_acc: 0.9987\n",
      "Epoch 40/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9982Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 288s - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0045 - val_acc: 0.9989\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9983Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0104 - val_acc: 0.9989\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9982Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 290s - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9988\n",
      "Epoch 43/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9982Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 289s - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0045 - val_acc: 0.9989\n",
      "Epoch 00042: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    image_dnn.fit(data_train, labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnn.load_weights('LAGAN{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnn.save_weights('LAGAN{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnn.load_weights('LAGAN{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with h5py.File('LAGAN{}vs{}-split-indices.h5'.format(CLASS_ONE, CLASS_TWO), 'w') as h5:\n",
    "#     h5['train'] = ix_train\n",
    "#     h5['test'] = ix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000/300000 [==============================] - 216s   \n"
     ]
    }
   ],
   "source": [
    "yhat_image_dnn = image_dnn.predict(data_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a DNN on shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_dnn(x):\n",
    "\n",
    "    h = Dense(256)(x)\n",
    "    h = Dropout(0.2)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "    \n",
    "    h = Dense(256)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(32)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "\n",
    "    h = Dense(1)(h)\n",
    "    y = Activation('sigmoid')(h)\n",
    "    \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(features_train.shape[1], ))\n",
    "feature_dnn = Model(x, build_feature_dnn(x))\n",
    "feature_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 215,105\n",
      "Trainable params: 213,057\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('SS{}vs{}-features-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9938Epoch 00000: val_loss improved from inf to 0.00950, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 72s - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0095 - val_acc: 0.9973\n",
      "Epoch 2/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9962Epoch 00001: val_loss improved from 0.00950 to 0.00864, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 64s - loss: 0.0140 - acc: 0.9962 - val_loss: 0.0086 - val_acc: 0.9975\n",
      "Epoch 3/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967Epoch 00002: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0125 - acc: 0.9967 - val_loss: 0.0133 - val_acc: 0.9971\n",
      "Epoch 4/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9970Epoch 00003: val_loss improved from 0.00864 to 0.00758, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0076 - val_acc: 0.9979\n",
      "Epoch 5/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9970Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0089 - val_acc: 0.9976\n",
      "Epoch 6/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 7/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9973Epoch 00006: val_loss improved from 0.00758 to 0.00721, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 64s - loss: 0.0103 - acc: 0.9973 - val_loss: 0.0072 - val_acc: 0.9979\n",
      "Epoch 8/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9974Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0100 - acc: 0.9974 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 9/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9973Epoch 00008: val_loss improved from 0.00721 to 0.00718, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0099 - acc: 0.9973 - val_loss: 0.0072 - val_acc: 0.9980\n",
      "Epoch 10/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9975Epoch 00009: val_loss improved from 0.00718 to 0.00697, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0096 - acc: 0.9975 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 11/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9975Epoch 00010: val_loss improved from 0.00697 to 0.00686, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 64s - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0069 - val_acc: 0.9981\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9976Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0071 - val_acc: 0.9981\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9975Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0080 - val_acc: 0.9976\n",
      "Epoch 14/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9976Epoch 00013: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 15/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0069 - val_acc: 0.9981\n",
      "Epoch 16/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9976Epoch 00015: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0071 - val_acc: 0.9980\n",
      "Epoch 17/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 18/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976Epoch 00017: val_loss improved from 0.00686 to 0.00660, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 19/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9977Epoch 00018: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0088 - acc: 0.9977 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 20/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977Epoch 00019: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0089 - acc: 0.9977 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 21/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978Epoch 00020: val_loss improved from 0.00660 to 0.00646, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 64s - loss: 0.0085 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 22/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 23/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9978Epoch 00022: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0087 - acc: 0.9978 - val_loss: 0.0068 - val_acc: 0.9981\n",
      "Epoch 24/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9978Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0086 - acc: 0.9977 - val_loss: 0.0068 - val_acc: 0.9980\n",
      "Epoch 25/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977Epoch 00024: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0086 - acc: 0.9977 - val_loss: 0.0067 - val_acc: 0.9981\n",
      "Epoch 26/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978Epoch 00025: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0085 - acc: 0.9978 - val_loss: 0.0069 - val_acc: 0.9981\n",
      "Epoch 27/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9979Epoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 28/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0086 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978Epoch 00028: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0085 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 0.9982\n",
      "Epoch 30/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978Epoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0068 - val_acc: 0.9980\n",
      "Epoch 31/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 0.9979\n",
      "Epoch 32/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978Epoch 00031: val_loss improved from 0.00646 to 0.00631, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0063 - val_acc: 0.9981\n",
      "Epoch 33/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9979Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0063 - val_acc: 0.9982\n",
      "Epoch 34/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9979Epoch 00033: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0083 - acc: 0.9979 - val_loss: 0.0064 - val_acc: 0.9982\n",
      "Epoch 35/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Epoch 00034: val_loss improved from 0.00631 to 0.00624, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0084 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 0.9982\n",
      "Epoch 37/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9978Epoch 00036: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0083 - acc: 0.9978 - val_loss: 0.0064 - val_acc: 0.9982\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9979Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0081 - acc: 0.9979 - val_loss: 0.0063 - val_acc: 0.9982\n",
      "Epoch 39/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0069 - val_acc: 0.9980\n",
      "Epoch 40/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0082 - acc: 0.9979 - val_loss: 0.0068 - val_acc: 0.9981\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0064 - val_acc: 0.9982\n",
      "Epoch 42/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9979Epoch 00041: val_loss improved from 0.00624 to 0.00620, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 64s - loss: 0.0083 - acc: 0.9979 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 43/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0065 - val_acc: 0.9982\n",
      "Epoch 44/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979Epoch 00043: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0082 - acc: 0.9979 - val_loss: 0.0074 - val_acc: 0.9980\n",
      "Epoch 45/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0069 - val_acc: 0.9981\n",
      "Epoch 46/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00045: val_loss did not improve\n",
      "489999/489999 [==============================] - 65s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 47/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Epoch 00046: val_loss improved from 0.00620 to 0.00611, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0061 - val_acc: 0.9983\n",
      "Epoch 48/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0079 - acc: 0.9979 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 50/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0079 - acc: 0.9979 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 51/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0067 - val_acc: 0.9981\n",
      "Epoch 52/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9980Epoch 00051: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0064 - val_acc: 0.9983\n",
      "Epoch 53/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0079 - acc: 0.9979 - val_loss: 0.0079 - val_acc: 0.9982\n",
      "Epoch 54/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979Epoch 00053: val_loss improved from 0.00611 to 0.00608, saving model to SSpiplusvseplus-features-chkpt.h5\n",
      "489999/489999 [==============================] - 65s - loss: 0.0079 - acc: 0.9979 - val_loss: 0.0061 - val_acc: 0.9982\n",
      "Epoch 55/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0079 - acc: 0.9979 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 56/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00055: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0070 - val_acc: 0.9981\n",
      "Epoch 57/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979Epoch 00056: val_loss did not improve\n",
      "489999/489999 [==============================] - 63s - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 58/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979Epoch 00057: val_loss did not improve\n",
      "489999/489999 [==============================] - 63s - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0063 - val_acc: 0.9982\n",
      "Epoch 59/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979Epoch 00058: val_loss did not improve\n",
      "489999/489999 [==============================] - 64s - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0065 - val_acc: 0.9983\n",
      "Epoch 60/100\n",
      " 20864/489999 [>.............................] - ETA: 58s - loss: 0.0090 - acc: 0.9979ending early\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    feature_dnn.fit(features_train / features_train.max(axis=0)[np.newaxis, :], labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dnn.load_weights('SS{}vs{}-features-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "feature_dnn.save_weights('SS{}vs{}-features-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dnn.load_weights('SS{}vs{}-features-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299968/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_feature_dnn = feature_dnn.predict(features_test / features_test.max(axis=0)[np.newaxis, :], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train a BDT on shower shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_clf = GradientBoostingClassifier(verbose=2)\n",
    "parameters = {\n",
    "    'n_estimators':[100, 200, 300],\n",
    "    'max_depth':[3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(base_clf, parameters, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_feature_bdt = clf.predict_proba(features_test)[:, 1].ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple DNN on pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_dnn(x):\n",
    "\n",
    "    h = Dense(512)(x)\n",
    "    h = Dropout(0.2)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(1024)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(2048)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "    \n",
    "    h = Dense(1024)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "    h = BatchNormalization()(h)\n",
    "\n",
    "    h = Dense(128)(h)\n",
    "    h = Dropout(0.5)(LeakyReLU()(h))\n",
    "\n",
    "    h = Dense(1)(h)\n",
    "    y = Activation('sigmoid')(h)\n",
    "    \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(raveled_train.shape[1], ))\n",
    "raveled_dnn = Model(x, build_simple_dnn(x))\n",
    "raveled_dnn.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 504)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               258560    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,131,009\n",
      "Trainable params: 5,121,793\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "raveled_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "    ModelCheckpoint('PixVec{}vs{}-raveled-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9932Epoch 00000: val_loss improved from inf to 0.01138, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0237 - acc: 0.9932 - val_loss: 0.0114 - val_acc: 0.9967\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9961Epoch 00001: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0120 - val_acc: 0.9965\n",
      "Epoch 3/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9966Epoch 00002: val_loss improved from 0.01138 to 0.00988, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0099 - val_acc: 0.9971\n",
      "Epoch 4/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9969Epoch 00003: val_loss improved from 0.00988 to 0.00775, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0077 - val_acc: 0.9977\n",
      "Epoch 5/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9972Epoch 00004: val_loss improved from 0.00775 to 0.00711, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0106 - acc: 0.9972 - val_loss: 0.0071 - val_acc: 0.9980\n",
      "Epoch 6/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9972Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0102 - acc: 0.9972 - val_loss: 0.0092 - val_acc: 0.9975\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9973Epoch 00006: val_loss improved from 0.00711 to 0.00684, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0099 - acc: 0.9973 - val_loss: 0.0068 - val_acc: 0.9979\n",
      "Epoch 8/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9974Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0096 - acc: 0.9974 - val_loss: 0.0077 - val_acc: 0.9979\n",
      "Epoch 9/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9974Epoch 00008: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0079 - val_acc: 0.9976\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9975Epoch 00009: val_loss improved from 0.00684 to 0.00653, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0091 - acc: 0.9975 - val_loss: 0.0065 - val_acc: 0.9982\n",
      "Epoch 11/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0079 - val_acc: 0.9980\n",
      "Epoch 12/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9975Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0067 - val_acc: 0.9980\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9976Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0094 - val_acc: 0.9977\n",
      "Epoch 14/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977Epoch 00013: val_loss improved from 0.00653 to 0.00652, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0084 - acc: 0.9977 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 15/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9976Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0077 - val_acc: 0.9979\n",
      "Epoch 16/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9977Epoch 00015: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0078 - val_acc: 0.9974\n",
      "Epoch 17/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9977Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0080 - acc: 0.9977 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 18/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978Epoch 00017: val_loss improved from 0.00652 to 0.00613, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0061 - val_acc: 0.9984\n",
      "Epoch 19/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978Epoch 00018: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0063 - val_acc: 0.9980\n",
      "Epoch 20/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9979Epoch 00019: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0063 - val_acc: 0.9980\n",
      "Epoch 21/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9980Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0072 - acc: 0.9980 - val_loss: 0.0083 - val_acc: 0.9980\n",
      "Epoch 22/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980Epoch 00021: val_loss improved from 0.00613 to 0.00526, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0053 - val_acc: 0.9985\n",
      "Epoch 23/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9980Epoch 00022: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0057 - val_acc: 0.9982\n",
      "Epoch 24/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9981Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0063 - val_acc: 0.9983\n",
      "Epoch 25/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981Epoch 00024: val_loss improved from 0.00526 to 0.00514, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0051 - val_acc: 0.9985\n",
      "Epoch 26/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9981Epoch 00025: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0052 - val_acc: 0.9986\n",
      "Epoch 27/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982Epoch 00026: val_loss improved from 0.00514 to 0.00474, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0047 - val_acc: 0.9986\n",
      "Epoch 28/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0054 - val_acc: 0.9985\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982Epoch 00028: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0055 - val_acc: 0.9986\n",
      "Epoch 30/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9982Epoch 00029: val_loss improved from 0.00474 to 0.00469, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0047 - val_acc: 0.9987\n",
      "Epoch 31/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9983Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0048 - val_acc: 0.9986\n",
      "Epoch 32/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9983Epoch 00031: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0050 - val_acc: 0.9986\n",
      "Epoch 33/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985Epoch 00032: val_loss improved from 0.00469 to 0.00457, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0046 - val_acc: 0.9987\n",
      "Epoch 34/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984Epoch 00033: val_loss improved from 0.00457 to 0.00454, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 69s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 35/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0048 - val_acc: 0.9985\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9983Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0059 - val_acc: 0.9987\n",
      "Epoch 37/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984Epoch 00036: val_loss improved from 0.00454 to 0.00439, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0044 - val_acc: 0.9988\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9985Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0044 - val_acc: 0.9987\n",
      "Epoch 39/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0045 - val_acc: 0.9988\n",
      "Epoch 40/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0048 - val_acc: 0.9986\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0056 - val_acc: 0.9986\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 43/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 70s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0047 - val_acc: 0.9987\n",
      "Epoch 44/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985Epoch 00043: val_loss improved from 0.00439 to 0.00438, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0044 - val_acc: 0.9988\n",
      "Epoch 45/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0049 - val_acc: 0.9986\n",
      "Epoch 46/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985Epoch 00045: val_loss improved from 0.00438 to 0.00423, saving model to PixVecpiplusvseplus-raveled-chkpt.h5\n",
      "489999/489999 [==============================] - 68s - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0042 - val_acc: 0.9989\n",
      "Epoch 47/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986Epoch 00046: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0051 - val_acc: 0.9986\n",
      "Epoch 48/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 70s - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0045 - val_acc: 0.9987\n",
      "Epoch 49/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0053 - val_acc: 0.9985\n",
      "Epoch 50/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0044 - val_acc: 0.9987\n",
      "Epoch 51/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0062 - val_acc: 0.9983\n",
      "Epoch 52/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986Epoch 00051: val_loss did not improve\n",
      "489999/489999 [==============================] - 69s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0066 - val_acc: 0.9984\n",
      "Epoch 53/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0048 - val_acc: 0.9986\n",
      "Epoch 54/100\n",
      "489856/489999 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986Epoch 00053: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0052 - val_acc: 0.9985\n",
      "Epoch 55/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0048 - val_acc: 0.9989\n",
      "Epoch 56/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9986Epoch 00055: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0052 - val_acc: 0.9988\n",
      "Epoch 57/100\n",
      "489600/489999 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986Epoch 00056: val_loss did not improve\n",
      "489999/489999 [==============================] - 68s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0050 - val_acc: 0.9987\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raveled_dnn.fit(raveled_train, labels_train, callbacks=callbacks, verbose=True,\n",
    "                          validation_split=0.3, batch_size=128, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_dnn.load_weights('PixVec{}vs{}-raveled-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "raveled_dnn.save_weights('PixVec{}vs{}-raveled-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raveled_dnn.load_weights('PixVec{}vs{}-raveled-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299936/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_raveled_dnn = raveled_dnn.predict(raveled_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mpaganini/keras-contrib/keras_contrib/applications\n",
      "/home/mpaganini/CaloGAN/classification\n"
     ]
    }
   ],
   "source": [
    "% cd ~/keras-contrib/keras_contrib/applications/\n",
    "% run densenet.py\n",
    "#from densenet import DenseNet as build_densenet\n",
    "% cd /home/mpaganini/CaloGAN/classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_densenet = DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to support 1 channel images, just comment out the input_shape = _obtain_input_shape part\n",
    "# just set the input_shape when you build the model (risky without checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet.py:507: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_regularizer=<keras.reg..., activation=\"sigmoid\", bias_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "dnet1 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(12, 12, 1), nb_dense_block=1,\n",
    "                      include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 12, 12, 16)    144         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 12, 12, 16)    64          initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 12, 12, 16)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 12, 12, 12)    1728        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 12, 12, 28)    0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 12, 12, 28)    112         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 12, 12, 28)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 12, 12, 12)    3024        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 12, 12, 40)    0           concatenate_1[0][0]              \n",
      "                                                                   conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 12, 12, 40)    160         concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 12, 12, 40)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 12, 12, 12)    4320        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 12, 12, 52)    0           concatenate_2[0][0]              \n",
      "                                                                   conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 12, 12, 52)    208         concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 12, 12, 52)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 12, 12, 12)    5616        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 12, 12, 64)    0           concatenate_3[0][0]              \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 12, 12, 64)    256         concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 12, 12, 64)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 12, 12, 12)    6912        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 12, 12, 76)    0           concatenate_4[0][0]              \n",
      "                                                                   conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 12, 12, 76)    304         concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 12, 12, 76)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 12, 12, 12)    8208        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 12, 12, 88)    0           concatenate_5[0][0]              \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 12, 12, 88)    352         concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 12, 12, 88)    0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 12, 12, 12)    9504        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 12, 12, 100)   0           concatenate_6[0][0]              \n",
      "                                                                   conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 12, 12, 100)   400         concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 12, 12, 100)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 12, 12, 12)    10800       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 12, 12, 112)   0           concatenate_7[0][0]              \n",
      "                                                                   conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 12, 12, 112)   448         concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 12, 12, 112)   0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 12, 12, 12)    12096       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 12, 12, 124)   0           concatenate_8[0][0]              \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 12, 12, 124)   496         concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 12, 12, 124)   0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 12, 12, 12)    13392       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)     (None, 12, 12, 136)   0           concatenate_9[0][0]              \n",
      "                                                                   conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 12, 12, 136)   544         concatenate_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 12, 12, 136)   0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 12, 12, 12)    14688       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)     (None, 12, 12, 148)   0           concatenate_10[0][0]             \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 12, 12, 148)   592         concatenate_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 12, 12, 148)   0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 12, 12, 12)    15984       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)     (None, 12, 12, 160)   0           concatenate_11[0][0]             \n",
      "                                                                   conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 12, 12, 160)   640         concatenate_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 12, 12, 160)   0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 160)           0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             161         global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 111,153\n",
      "Trainable params: 108,865\n",
      "Non-trainable params: 2,288\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_model(dnet, to_file='dnet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='dnet.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(verbose=True, patience=20, monitor='val_loss'),\n",
    "    ModelCheckpoint('DenseNet1{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9937Epoch 00000: val_loss improved from inf to 0.08401, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 127s - loss: 0.0304 - acc: 0.9937 - val_loss: 0.0840 - val_acc: 0.9794\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9938Epoch 00001: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0300 - acc: 0.9938 - val_loss: 0.1758 - val_acc: 0.9652\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9939Epoch 00002: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0294 - acc: 0.9939 - val_loss: 1.3075 - val_acc: 0.7699\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9940Epoch 00003: val_loss improved from 0.08401 to 0.05300, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0288 - acc: 0.9940 - val_loss: 0.0530 - val_acc: 0.9866\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9939Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0288 - acc: 0.9939 - val_loss: 1.3921 - val_acc: 0.7879\n",
      "Epoch 6/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9939Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0286 - acc: 0.9939 - val_loss: 0.1766 - val_acc: 0.9405\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9939Epoch 00006: val_loss improved from 0.05300 to 0.04646, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 125s - loss: 0.0282 - acc: 0.9939 - val_loss: 0.0465 - val_acc: 0.9862\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9939Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0284 - acc: 0.9939 - val_loss: 0.0933 - val_acc: 0.9726\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9941Epoch 00008: val_loss improved from 0.04646 to 0.03845, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0385 - val_acc: 0.9902\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9941Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0537 - val_acc: 0.9852\n",
      "Epoch 11/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9939Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0279 - acc: 0.9939 - val_loss: 0.2248 - val_acc: 0.9220\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9941Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0275 - acc: 0.9941 - val_loss: 0.1564 - val_acc: 0.9472\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9940Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 125s - loss: 0.0275 - acc: 0.9940 - val_loss: 0.2013 - val_acc: 0.9516\n",
      "Epoch 14/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9941Epoch 00013: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0274 - acc: 0.9941 - val_loss: 0.5369 - val_acc: 0.9107\n",
      "Epoch 15/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9942Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0271 - acc: 0.9942 - val_loss: 0.1172 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9941Epoch 00015: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0270 - acc: 0.9941 - val_loss: 0.3817 - val_acc: 0.9323\n",
      "Epoch 17/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9942Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0272 - acc: 0.9942 - val_loss: 0.8494 - val_acc: 0.7958\n",
      "Epoch 18/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9942Epoch 00017: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0267 - acc: 0.9942 - val_loss: 0.0941 - val_acc: 0.9779\n",
      "Epoch 19/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9941Epoch 00018: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0271 - acc: 0.9941 - val_loss: 0.2104 - val_acc: 0.9416\n",
      "Epoch 20/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9942Epoch 00019: val_loss improved from 0.03845 to 0.03463, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0266 - acc: 0.9942 - val_loss: 0.0346 - val_acc: 0.9918\n",
      "Epoch 21/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9942Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0265 - acc: 0.9942 - val_loss: 0.0352 - val_acc: 0.9906\n",
      "Epoch 22/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9941Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0269 - acc: 0.9941 - val_loss: 0.1083 - val_acc: 0.9786\n",
      "Epoch 23/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9943Epoch 00022: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9943 - val_loss: 0.4564 - val_acc: 0.8818\n",
      "Epoch 24/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0544 - val_acc: 0.9873\n",
      "Epoch 25/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00024: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0265 - acc: 0.9942 - val_loss: 0.3428 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9940Epoch 00025: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0270 - acc: 0.9940 - val_loss: 0.4166 - val_acc: 0.8935\n",
      "Epoch 27/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9942Epoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0262 - acc: 0.9942 - val_loss: 0.0407 - val_acc: 0.9897\n",
      "Epoch 28/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9942Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0261 - acc: 0.9942 - val_loss: 0.2992 - val_acc: 0.8967\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9943Epoch 00028: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0262 - acc: 0.9943 - val_loss: 0.2714 - val_acc: 0.9292\n",
      "Epoch 30/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9943Epoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0260 - acc: 0.9943 - val_loss: 0.1535 - val_acc: 0.9697\n",
      "Epoch 31/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9943Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9943 - val_loss: 0.2747 - val_acc: 0.9565\n",
      "Epoch 32/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00031: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.0946 - val_acc: 0.9695\n",
      "Epoch 33/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.0800 - val_acc: 0.9787\n",
      "Epoch 34/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9943Epoch 00033: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9943 - val_loss: 0.9192 - val_acc: 0.7246\n",
      "Epoch 35/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9921Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0336 - acc: 0.9921 - val_loss: 0.0402 - val_acc: 0.9900\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9940Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0272 - acc: 0.9940 - val_loss: 0.0556 - val_acc: 0.9855\n",
      "Epoch 37/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9942Epoch 00036: val_loss improved from 0.03463 to 0.03341, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0334 - val_acc: 0.9928\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9942Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0260 - acc: 0.9942 - val_loss: 0.0418 - val_acc: 0.9896\n",
      "Epoch 39/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9943Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9943 - val_loss: 0.1422 - val_acc: 0.9490\n",
      "Epoch 40/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9944Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0257 - acc: 0.9944 - val_loss: 0.1183 - val_acc: 0.9590\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9943Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9943 - val_loss: 0.0989 - val_acc: 0.9721\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9943Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9943 - val_loss: 0.7811 - val_acc: 0.7953\n",
      "Epoch 43/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9944Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9944 - val_loss: 0.1129 - val_acc: 0.9623\n",
      "Epoch 44/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00043: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0254 - acc: 0.9944 - val_loss: 0.0447 - val_acc: 0.9880\n",
      "Epoch 45/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0255 - acc: 0.9944 - val_loss: 1.8572 - val_acc: 0.7730\n",
      "Epoch 46/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9942Epoch 00045: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0261 - acc: 0.9942 - val_loss: 0.1687 - val_acc: 0.9604\n",
      "Epoch 47/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9942Epoch 00046: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0258 - acc: 0.9942 - val_loss: 0.6475 - val_acc: 0.8973\n",
      "Epoch 48/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9944Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0253 - acc: 0.9944 - val_loss: 0.1166 - val_acc: 0.9707\n",
      "Epoch 49/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9944Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0253 - acc: 0.9944 - val_loss: 0.0388 - val_acc: 0.9888\n",
      "Epoch 50/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 1.0034 - val_acc: 0.7657\n",
      "Epoch 51/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.0913 - val_acc: 0.9815\n",
      "Epoch 52/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9944Epoch 00051: val_loss improved from 0.03341 to 0.03240, saving model to DenseNet1piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9944 - val_loss: 0.0324 - val_acc: 0.9927\n",
      "Epoch 53/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9944Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0254 - acc: 0.9944 - val_loss: 0.2112 - val_acc: 0.9637\n",
      "Epoch 54/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945Epoch 00053: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.0764 - val_acc: 0.9819\n",
      "Epoch 55/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9942Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0256 - acc: 0.9941 - val_loss: 2.0979 - val_acc: 0.5144\n",
      "Epoch 56/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9935Epoch 00055: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0285 - acc: 0.9935 - val_loss: 0.1472 - val_acc: 0.9562\n",
      "Epoch 57/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9945Epoch 00056: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9945 - val_loss: 0.3062 - val_acc: 0.9372\n",
      "Epoch 58/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00057: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.9211 - val_acc: 0.7731\n",
      "Epoch 59/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9943Epoch 00058: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0251 - acc: 0.9943 - val_loss: 1.0202 - val_acc: 0.7124\n",
      "Epoch 60/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00059: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.4974 - val_acc: 0.8871\n",
      "Epoch 61/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00060: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.4810 - val_acc: 0.9043\n",
      "Epoch 62/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9944Epoch 00061: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0250 - acc: 0.9944 - val_loss: 0.1112 - val_acc: 0.9732\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9942Epoch 00062: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0259 - acc: 0.9942 - val_loss: 1.5563 - val_acc: 0.5760\n",
      "Epoch 64/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9945Epoch 00063: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0249 - acc: 0.9945 - val_loss: 0.9741 - val_acc: 0.7414\n",
      "Epoch 65/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9944Epoch 00064: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0252 - acc: 0.9944 - val_loss: 0.0468 - val_acc: 0.9873\n",
      "Epoch 66/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9944Epoch 00065: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9944 - val_loss: 1.6008 - val_acc: 0.6975\n",
      "Epoch 67/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9944Epoch 00066: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0251 - acc: 0.9944 - val_loss: 2.1769 - val_acc: 0.6182\n",
      "Epoch 68/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9945Epoch 00067: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9945 - val_loss: 0.0353 - val_acc: 0.9914\n",
      "Epoch 69/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945Epoch 00068: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.0630 - val_acc: 0.9853\n",
      "Epoch 70/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9944Epoch 00069: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0248 - acc: 0.9944 - val_loss: 0.0496 - val_acc: 0.9871\n",
      "Epoch 71/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9938Epoch 00070: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0273 - acc: 0.9938 - val_loss: 0.2112 - val_acc: 0.9477\n",
      "Epoch 72/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9946Epoch 00071: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9946 - val_loss: 0.7036 - val_acc: 0.8278\n",
      "Epoch 73/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9945Epoch 00072: val_loss did not improve\n",
      "489999/489999 [==============================] - 126s - loss: 0.0247 - acc: 0.9945 - val_loss: 0.0575 - val_acc: 0.9868\n",
      "Epoch 00072: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the middle layer\n",
    "try:\n",
    "    dnet1.fit(data_train[1], labels_train, callbacks=callbacks, verbose=True,\n",
    "                  validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.load_weights('DenseNet1{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet1.save_weights('DenseNet1{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet1.load_weights('DenseNet1{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299872/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet = dnet1.predict(data_test[1], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_densenet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:508: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_regularizer=<keras.reg..., activation=\"sigmoid\", bias_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "dnet0 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(3, 96, 1),\n",
    "                       nb_dense_block=1, bottleneck=False) \n",
    "# ugly shapes, but ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 3, 96, 16)     144         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 3, 96, 16)     0           initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 3, 96, 12)     1728        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 3, 96, 28)     0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 3, 96, 28)     0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 3, 96, 12)     3024        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 3, 96, 40)     0           concatenate_1[0][0]              \n",
      "                                                                   conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 3, 96, 40)     0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 3, 96, 12)     4320        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 3, 96, 52)     0           concatenate_2[0][0]              \n",
      "                                                                   conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 3, 96, 52)     0           concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 3, 96, 12)     5616        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 3, 96, 64)     0           concatenate_3[0][0]              \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 3, 96, 64)     0           concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 3, 96, 12)     6912        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 3, 96, 76)     0           concatenate_4[0][0]              \n",
      "                                                                   conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 3, 96, 76)     0           concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 3, 96, 12)     8208        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 3, 96, 88)     0           concatenate_5[0][0]              \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 3, 96, 88)     0           concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 3, 96, 12)     9504        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 3, 96, 100)    0           concatenate_6[0][0]              \n",
      "                                                                   conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 3, 96, 100)    0           concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 3, 96, 12)     10800       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 3, 96, 112)    0           concatenate_7[0][0]              \n",
      "                                                                   conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 3, 96, 112)    0           concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 3, 96, 12)     12096       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 3, 96, 124)    0           concatenate_8[0][0]              \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 3, 96, 124)    0           concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 3, 96, 12)     13392       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)     (None, 3, 96, 136)    0           concatenate_9[0][0]              \n",
      "                                                                   conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 3, 96, 136)    0           concatenate_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 3, 96, 12)     14688       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)     (None, 3, 96, 148)    0           concatenate_10[0][0]             \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 3, 96, 148)    0           concatenate_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 3, 96, 12)     15984       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)     (None, 3, 96, 160)    0           concatenate_11[0][0]             \n",
      "                                                                   conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 3, 96, 160)    640         concatenate_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 3, 96, 160)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 160)           0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             161         global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 107,217\n",
      "Trainable params: 106,897\n",
      "Non-trainable params: 320\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet0.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9781Epoch 00000: val_loss improved from inf to 0.07470, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 103s - loss: 0.1055 - acc: 0.9781 - val_loss: 0.0747 - val_acc: 0.9896\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9894Epoch 00001: val_loss improved from 0.07470 to 0.05956, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0637 - acc: 0.9894 - val_loss: 0.0596 - val_acc: 0.9907\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9911- ETA: 5s - lEpoch 00002: val_loss improved from 0.05956 to 0.05158, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0558 - acc: 0.9911 - val_loss: 0.0516 - val_acc: 0.9924\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9921Epoch 00003: val_loss improved from 0.05158 to 0.04922, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0505 - acc: 0.9921 - val_loss: 0.0492 - val_acc: 0.9928\n",
      "Epoch 5/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9926- ETA: 4s - loss: 0.0482 - aEpoch 00004: val_loss improved from 0.04922 to 0.04667, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0480 - acc: 0.9926 - val_loss: 0.0467 - val_acc: 0.9928\n",
      "Epoch 6/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9929Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0459 - acc: 0.9929 - val_loss: 0.0661 - val_acc: 0.9885\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9930Epoch 00006: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0439 - acc: 0.9930 - val_loss: 0.0799 - val_acc: 0.9856\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9933Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 105s - loss: 0.0430 - acc: 0.9933 - val_loss: 0.0507 - val_acc: 0.9911\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9933Epoch 00008: val_loss improved from 0.04667 to 0.04194, saving model to DenseNet0piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 105s - loss: 0.0418 - acc: 0.9933 - val_loss: 0.0419 - val_acc: 0.9931\n",
      "Epoch 10/100\n",
      " 26624/489999 [>.............................] - ETA: 87s - loss: 0.0397 - acc: 0.9939ending early\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the first layer\n",
    "try:\n",
    "    dnet0.fit(data_train[0], labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet0{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet0.load_weights('DenseNet0{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet0.save_weights('DenseNet0{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet0.load_weights('DenseNet0{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299552/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet0 = dnet0.predict(data_test[0], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2 = build_densenet(weights=None, classes=1, activation='sigmoid',\n",
    "                       input_shape=(12, 6, 1), nb_dense_block=1, bottleneck=False)\n",
    "# it doesn't work with nb_dense_block=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv2D)          (None, 12, 6, 16)     144         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 12, 6, 16)     0           initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 12, 6, 12)     1728        activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)     (None, 12, 6, 28)     0           initial_conv2D[0][0]             \n",
      "                                                                   conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 12, 6, 28)     0           concatenate_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 12, 6, 12)     3024        activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 12, 6, 40)     0           concatenate_13[0][0]             \n",
      "                                                                   conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 12, 6, 40)     0           concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 12, 6, 12)     4320        activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 12, 6, 52)     0           concatenate_14[0][0]             \n",
      "                                                                   conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 12, 6, 52)     0           concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 12, 6, 12)     5616        activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 12, 6, 64)     0           concatenate_15[0][0]             \n",
      "                                                                   conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 12, 6, 64)     0           concatenate_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 12, 6, 12)     6912        activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)     (None, 12, 6, 76)     0           concatenate_16[0][0]             \n",
      "                                                                   conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 12, 6, 76)     0           concatenate_17[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 12, 6, 12)     8208        activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)     (None, 12, 6, 88)     0           concatenate_17[0][0]             \n",
      "                                                                   conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 12, 6, 88)     0           concatenate_18[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 12, 6, 12)     9504        activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)     (None, 12, 6, 100)    0           concatenate_18[0][0]             \n",
      "                                                                   conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 12, 6, 100)    0           concatenate_19[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 12, 6, 12)     10800       activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 12, 6, 112)    0           concatenate_19[0][0]             \n",
      "                                                                   conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 12, 6, 112)    0           concatenate_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 12, 6, 12)     12096       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)     (None, 12, 6, 124)    0           concatenate_20[0][0]             \n",
      "                                                                   conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 12, 6, 124)    0           concatenate_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 12, 6, 12)     13392       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 12, 6, 136)    0           concatenate_21[0][0]             \n",
      "                                                                   conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 12, 6, 136)    0           concatenate_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 12, 6, 12)     14688       activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 12, 6, 148)    0           concatenate_22[0][0]             \n",
      "                                                                   conv2d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 12, 6, 148)    0           concatenate_23[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, 12, 6, 12)     15984       activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)     (None, 12, 6, 160)    0           concatenate_23[0][0]             \n",
      "                                                                   conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 12, 6, 160)    640         concatenate_24[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 12, 6, 160)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 160)           0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             161         global_average_pooling2d_2[0][0] \n",
      "====================================================================================================\n",
      "Total params: 107,217\n",
      "Trainable params: 106,897\n",
      "Non-trainable params: 320\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnet2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9456Epoch 00000: val_loss improved from inf to 0.15739, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.2031 - acc: 0.9456 - val_loss: 0.1574 - val_acc: 0.9632\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9617Epoch 00001: val_loss improved from 0.15739 to 0.14472, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1602 - acc: 0.9617 - val_loss: 0.1447 - val_acc: 0.9664\n",
      "Epoch 3/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9638Epoch 00002: val_loss improved from 0.14472 to 0.13554, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1494 - acc: 0.9638 - val_loss: 0.1355 - val_acc: 0.9672\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9655Epoch 00003: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1418 - acc: 0.9655 - val_loss: 0.1396 - val_acc: 0.9659\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9661Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1365 - acc: 0.9661 - val_loss: 0.1362 - val_acc: 0.9629\n",
      "Epoch 6/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9663Epoch 00005: val_loss improved from 0.13554 to 0.12710, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1349 - acc: 0.9663 - val_loss: 0.1271 - val_acc: 0.9678\n",
      "Epoch 7/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9664Epoch 00006: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1329 - acc: 0.9664 - val_loss: 0.1329 - val_acc: 0.9675\n",
      "Epoch 8/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9675Epoch 00007: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1302 - acc: 0.9675 - val_loss: 0.1355 - val_acc: 0.9616\n",
      "Epoch 9/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9674Epoch 00008: val_loss improved from 0.12710 to 0.12248, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1289 - acc: 0.9674 - val_loss: 0.1225 - val_acc: 0.9696\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9678Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1279 - acc: 0.9678 - val_loss: 0.1287 - val_acc: 0.9686\n",
      "Epoch 11/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9679Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1267 - acc: 0.9679 - val_loss: 0.1320 - val_acc: 0.9630\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9685Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1246 - acc: 0.9685 - val_loss: 0.1247 - val_acc: 0.9696\n",
      "Epoch 13/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9680Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1255 - acc: 0.9680 - val_loss: 0.1514 - val_acc: 0.9580\n",
      "Epoch 14/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9680Epoch 00013: val_loss improved from 0.12248 to 0.12098, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 42s - loss: 0.1261 - acc: 0.9679 - val_loss: 0.1210 - val_acc: 0.9698\n",
      "Epoch 15/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9690Epoch 00014: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1218 - acc: 0.9690 - val_loss: 0.1330 - val_acc: 0.9629\n",
      "Epoch 16/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9691Epoch 00015: val_loss improved from 0.12098 to 0.11966, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1216 - acc: 0.9691 - val_loss: 0.1197 - val_acc: 0.9702\n",
      "Epoch 17/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9694Epoch 00016: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1201 - acc: 0.9694 - val_loss: 0.1249 - val_acc: 0.9658\n",
      "Epoch 18/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9685Epoch 00017: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1224 - acc: 0.9685 - val_loss: 0.1319 - val_acc: 0.9631\n",
      "Epoch 19/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9699Epoch 00018: val_loss improved from 0.11966 to 0.11797, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1188 - acc: 0.9699 - val_loss: 0.1180 - val_acc: 0.9707\n",
      "Epoch 20/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9705Epoch 00019: val_loss improved from 0.11797 to 0.11297, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1159 - acc: 0.9705 - val_loss: 0.1130 - val_acc: 0.9715\n",
      "Epoch 21/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9703Epoch 00020: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1161 - acc: 0.9703 - val_loss: 0.1178 - val_acc: 0.9698\n",
      "Epoch 22/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9705Epoch 00021: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1156 - acc: 0.9705 - val_loss: 0.1321 - val_acc: 0.9633\n",
      "Epoch 23/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9713Epoch 00022: val_loss improved from 0.11297 to 0.10589, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1123 - acc: 0.9713 - val_loss: 0.1059 - val_acc: 0.9729\n",
      "Epoch 24/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9715Epoch 00023: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1117 - acc: 0.9715 - val_loss: 0.1862 - val_acc: 0.9488\n",
      "Epoch 25/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9717Epoch 00024: val_loss improved from 0.10589 to 0.10536, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1110 - acc: 0.9717 - val_loss: 0.1054 - val_acc: 0.9730\n",
      "Epoch 26/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9722Epoch 00025: val_loss improved from 0.10536 to 0.10183, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1087 - acc: 0.9722 - val_loss: 0.1018 - val_acc: 0.9738\n",
      "Epoch 27/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9722Epoch 00026: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1084 - acc: 0.9722 - val_loss: 0.1033 - val_acc: 0.9732\n",
      "Epoch 28/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9723Epoch 00027: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1089 - acc: 0.9723 - val_loss: 0.1093 - val_acc: 0.9707\n",
      "Epoch 29/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9726Epoch 00028: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489999/489999 [==============================] - 41s - loss: 0.1073 - acc: 0.9726 - val_loss: 0.1173 - val_acc: 0.9672\n",
      "Epoch 30/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9727Epoch 00029: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1067 - acc: 0.9727 - val_loss: 0.1265 - val_acc: 0.9655\n",
      "Epoch 31/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9726Epoch 00030: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1071 - acc: 0.9726 - val_loss: 0.1445 - val_acc: 0.9627\n",
      "Epoch 32/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9729Epoch 00031: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1058 - acc: 0.9729 - val_loss: 0.1066 - val_acc: 0.9730\n",
      "Epoch 33/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9729Epoch 00032: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1059 - acc: 0.9729 - val_loss: 0.1095 - val_acc: 0.9722\n",
      "Epoch 34/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9731Epoch 00033: val_loss improved from 0.10183 to 0.10149, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 42s - loss: 0.1057 - acc: 0.9731 - val_loss: 0.1015 - val_acc: 0.9731\n",
      "Epoch 35/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9731Epoch 00034: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1053 - acc: 0.9731 - val_loss: 0.1044 - val_acc: 0.9725\n",
      "Epoch 36/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9732Epoch 00035: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1047 - acc: 0.9732 - val_loss: 0.2070 - val_acc: 0.9302\n",
      "Epoch 37/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9732Epoch 00036: val_loss improved from 0.10149 to 0.09876, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1048 - acc: 0.9732 - val_loss: 0.0988 - val_acc: 0.9742\n",
      "Epoch 38/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9734Epoch 00037: val_loss did not improve\n",
      "489999/489999 [==============================] - 42s - loss: 0.1045 - acc: 0.9734 - val_loss: 0.1014 - val_acc: 0.9731\n",
      "Epoch 39/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9731Epoch 00038: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1049 - acc: 0.9731 - val_loss: 0.1051 - val_acc: 0.9716\n",
      "Epoch 40/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9731Epoch 00039: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1052 - acc: 0.9731 - val_loss: 0.1206 - val_acc: 0.9678\n",
      "Epoch 41/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9734Epoch 00040: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1045 - acc: 0.9734 - val_loss: 0.1157 - val_acc: 0.9682\n",
      "Epoch 42/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9729Epoch 00041: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1051 - acc: 0.9729 - val_loss: 0.1054 - val_acc: 0.9724\n",
      "Epoch 43/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9735Epoch 00042: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9735 - val_loss: 0.1460 - val_acc: 0.9603\n",
      "Epoch 44/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9734Epoch 00043: val_loss improved from 0.09876 to 0.09765, saving model to DenseNet2piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 41s - loss: 0.1039 - acc: 0.9734 - val_loss: 0.0977 - val_acc: 0.9746\n",
      "Epoch 45/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9736Epoch 00044: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1030 - acc: 0.9736 - val_loss: 0.0988 - val_acc: 0.9749\n",
      "Epoch 46/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9735Epoch 00045: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9735 - val_loss: 0.1022 - val_acc: 0.9731\n",
      "Epoch 47/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9734Epoch 00046: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1038 - acc: 0.9734 - val_loss: 0.0978 - val_acc: 0.9746\n",
      "Epoch 48/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9732Epoch 00047: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1044 - acc: 0.9732 - val_loss: 0.1160 - val_acc: 0.9687\n",
      "Epoch 49/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9736Epoch 00048: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9736 - val_loss: 0.1053 - val_acc: 0.9737\n",
      "Epoch 50/100\n",
      "489728/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9735Epoch 00049: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1037 - acc: 0.9735 - val_loss: 0.1104 - val_acc: 0.9701\n",
      "Epoch 51/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9734Epoch 00050: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9734 - val_loss: 0.1087 - val_acc: 0.9720\n",
      "Epoch 52/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9734Epoch 00051: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1038 - acc: 0.9733 - val_loss: 0.1129 - val_acc: 0.9716\n",
      "Epoch 53/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9734Epoch 00052: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1033 - acc: 0.9734 - val_loss: 0.1289 - val_acc: 0.9654\n",
      "Epoch 54/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9735Epoch 00053: val_loss did not improve\n",
      "489999/489999 [==============================] - 40s - loss: 0.1035 - acc: 0.9735 - val_loss: 0.1029 - val_acc: 0.9725\n",
      "Epoch 55/100\n",
      "489472/489999 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9733Epoch 00054: val_loss did not improve\n",
      "489999/489999 [==============================] - 41s - loss: 0.1031 - acc: 0.9734 - val_loss: 0.1045 - val_acc: 0.9720\n",
      "Epoch 00054: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a Dense Net only on the last layer\n",
    "try:\n",
    "    dnet2.fit(data_train[2], labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet2{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet2.load_weights('DenseNet2{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet2.save_weights('DenseNet2{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "dnet2.load_weights('DenseNet2{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299744/300000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "yhat_dnet2 = dnet2.predict(data_test[2], verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge DenseNets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapes = [d.shape[1:] for d in data_train]\n",
    "\n",
    "x = [Input(shape=sh) for sh in shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnet_layer0 = build_densenet(weights=None, input_shape=(3, 96, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_layer1 = build_densenet(weights=None, input_shape=(12, 12, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_layer2 = build_densenet(weights=None, input_shape=(12, 6, 1), nb_dense_block=1,\n",
    "                      include_top=False)\n",
    "dnet_merged = [dnet_layer0, dnet_layer1, dnet_layer2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [f(xi) for f, xi in zip(dnet_merged, x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Dense(1, activation='sigmoid')(Concatenate()(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 3, 96, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 12, 12, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 12, 6, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_3 (Model)                  (None, 160)           107056      input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_4 (Model)                  (None, 160)           107056      input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_5 (Model)                  (None, 160)           107056      input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)     (None, 480)           0           model_3[1][0]                    \n",
      "                                                                   model_4[1][0]                    \n",
      "                                                                   model_5[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             481         concatenate_61[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 321,649\n",
      "Trainable params: 320,689\n",
      "Non-trainable params: 960\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_dnet_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged.compile('adam', 'binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489999 samples, validate on 210001 samples\n",
      "Epoch 1/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9890Epoch 00000: val_loss improved from inf to 0.05579, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 189s - loss: 0.0991 - acc: 0.9890 - val_loss: 0.0558 - val_acc: 0.9958\n",
      "Epoch 2/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9959Epoch 00001: val_loss improved from 0.05579 to 0.04818, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 189s - loss: 0.0441 - acc: 0.9959 - val_loss: 0.0482 - val_acc: 0.9951\n",
      "Epoch 3/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9967Epoch 00002: val_loss improved from 0.04818 to 0.02880, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 189s - loss: 0.0339 - acc: 0.9967 - val_loss: 0.0288 - val_acc: 0.9969\n",
      "Epoch 4/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9969Epoch 00003: val_loss did not improve\n",
      "489999/489999 [==============================] - 189s - loss: 0.0291 - acc: 0.9969 - val_loss: 0.0479 - val_acc: 0.9936\n",
      "Epoch 5/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9964Epoch 00004: val_loss did not improve\n",
      "489999/489999 [==============================] - 189s - loss: 0.0313 - acc: 0.9964 - val_loss: 0.0399 - val_acc: 0.9950\n",
      "Epoch 6/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9970Epoch 00005: val_loss did not improve\n",
      "489999/489999 [==============================] - 189s - loss: 0.0271 - acc: 0.9970 - val_loss: 0.0407 - val_acc: 0.9939\n",
      "Epoch 7/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9971Epoch 00006: val_loss improved from 0.02880 to 0.02231, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 189s - loss: 0.0250 - acc: 0.9971 - val_loss: 0.0223 - val_acc: 0.9977\n",
      "Epoch 8/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9972Epoch 00007: val_loss improved from 0.02231 to 0.02191, saving model to DenseNet_noBN_merged_piplusvseplus-chkpt.h5\n",
      "489999/489999 [==============================] - 189s - loss: 0.0236 - acc: 0.9972 - val_loss: 0.0219 - val_acc: 0.9979\n",
      "Epoch 9/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9973Epoch 00008: val_loss did not improve\n",
      "489999/489999 [==============================] - 189s - loss: 0.0225 - acc: 0.9973 - val_loss: 0.0314 - val_acc: 0.9968\n",
      "Epoch 10/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9974Epoch 00009: val_loss did not improve\n",
      "489999/489999 [==============================] - 188s - loss: 0.0215 - acc: 0.9974 - val_loss: 0.0234 - val_acc: 0.9973\n",
      "Epoch 11/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9974Epoch 00010: val_loss did not improve\n",
      "489999/489999 [==============================] - 188s - loss: 0.0209 - acc: 0.9974 - val_loss: 0.0277 - val_acc: 0.9962\n",
      "Epoch 12/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9974Epoch 00011: val_loss did not improve\n",
      "489999/489999 [==============================] - 188s - loss: 0.0206 - acc: 0.9974 - val_loss: 0.0229 - val_acc: 0.9979\n",
      "Epoch 13/100\n",
      "489984/489999 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9975Epoch 00012: val_loss did not improve\n",
      "489999/489999 [==============================] - 189s - loss: 0.0199 - acc: 0.9975 - val_loss: 0.0247 - val_acc: 0.9977\n",
      "Epoch 14/100\n",
      " 22784/489999 [>.............................] - ETA: 158s - loss: 0.0220 - acc: 0.9970ending early\n"
     ]
    }
   ],
   "source": [
    "# Train a merged Dense Net\n",
    "try:\n",
    "    image_dnet_merged.fit(data_train, labels_train, \n",
    "              callbacks=[\n",
    "                  EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "                  ModelCheckpoint('DenseNet_noBN_merged_{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO),\n",
    "                    monitor='val_loss', verbose=True, save_best_only=True)\n",
    "              ],\n",
    "              verbose=True, validation_split=0.3, batch_size=256, epochs=100)\n",
    "except KeyboardInterrupt:\n",
    "    print 'ending early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dnet_merged.load_weights('DenseNet_noBN_merged_{}vs{}-chkpt.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnet_merged.save_weights('DenseNet_noBN_merged_{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))\n",
    "image_dnet_merged.load_weights('DenseNet_noBN_merged_{}vs{}-final.h5'.format(CLASS_ONE, CLASS_TWO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000/300000 [==============================] - 70s    \n"
     ]
    }
   ],
   "source": [
    "yhat_dnet_merged = image_dnet_merged.predict(data_test, verbose=True).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2f8f4dd0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAJVCAYAAADtOuA8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqT8/YBUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeb94a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "bins = np.linspace(0, 1, 100)\n",
    "# plt.hist(yhat_image_dnn[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, Image DNN', color='red')\n",
    "# plt.hist(yhat_image_dnn[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, Image DNN', color='red',\n",
    "#         linestyle='dashed')\n",
    "plt.hist(yhat_dnet_merged[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, DenseNet', color='purple')\n",
    "plt.hist(yhat_dnet_merged[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, DenseNet', color='purple',\n",
    "        linestyle='dashed')\n",
    "plt.hist(yhat_feature_dnn[labels_test == 1], histtype='step', bins=bins, label=r'$\\pi^{+}$, Feature DNN')\n",
    "plt.hist(yhat_feature_dnn[labels_test == 0], histtype='step', bins=bins, label=r'$e^{+}$, Feature DNN')\n",
    "plt.legend(loc='upper center')\n",
    "plt.yscale('log')\n",
    "# plt.xlim((0.99999, 1))\n",
    "# plt.xscale('log')\n",
    "plt.xlabel(r'$\\mathbb{P}[\\pi^{+}]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_image_dnn, tpr_image_dnn, _ = roc_curve(labels_test, abs(1-yhat_image_dnn), pos_label=0)\n",
    "# fpr_dnet, tpr_dnet, _ = roc_curve(labels_test, abs(1-yhat_dnet), pos_label=0)\n",
    "fpr_dnet0, tpr_dnet0, _ = roc_curve(labels_test, abs(1-yhat_dnet0), pos_label=0)\n",
    "fpr_dnet2, tpr_dnet2, _ = roc_curve(labels_test, abs(1-yhat_dnet2), pos_label=0)\n",
    "# fpr_dnet_mean, tpr_dnet_mean, _ = roc_curve(labels_test,\n",
    "#                                             abs(1 - ((yhat_dnet2 + yhat_dnet + yhat_dnet0)/3) ),\n",
    "#                                             pos_label=0)\n",
    "\n",
    "fpr_dnet_merged, tpr_dnet_merged, _ = roc_curve(labels_test, abs(1-yhat_dnet_merged), pos_label=0)\n",
    "# fpr_raveled_dnn, tpr_raveled_dnn, _ = roc_curve(labels_test, abs(1-yhat_raveled_dnn), pos_label=0)\n",
    "# fpr_feature_dnn, tpr_feature_dnn, _ = roc_curve(labels_test, abs(1-yhat_feature_dnn), pos_label=0)\n",
    "# fpr_feature_bdt, tpr_feature_bdt, _ = roc_curve(labels_test, yhat_feature_bdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('{}-vs-{}-outputs.h5'.format(CLASS_ONE, CLASS_TWO), 'w') as h5:\n",
    "    h5['y'] = labels_test\n",
    "    h5['nn_image'] = yhat_image_dnn\n",
    "    h5['nn_raveled'] = yhat_raveled_dnn\n",
    "    h5['nn_showershapes'] = yhat_feature_dnn\n",
    "    h5['bdt_showershapes'] = yhat_feature_bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slices = np.linspace(0, 1, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpr, _ = np.histogram(yhat[labels_test == 1], bins=slices)\n",
    "fpr, _ = np.histogram(yhat[labels_test == 0], bins=slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpr = np.cumsum(tpr[::-1])[::-1]\n",
    "fpr = np.cumsum(fpr[::-1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  after removing the cwd from sys.path.\n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in divide\n",
      "  import sys\n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3a0e8f10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJQCAYAAADolpLRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzNrA/8DQ1H9tg5LZI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29341850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_image_dnn, 1 / fpr_image_dnn, label='LAGAN-Style Discriminator')\n",
    "plt.plot(tpr_feature_dnn, 1 / fpr_feature_dnn, label='DNN on Shower Shapes')\n",
    "plt.plot(tpr_raveled_dnn, 1 / fpr_raveled_dnn, label='DNN on Raveled Pixels')\n",
    "\n",
    "plt.plot(tpr_dnet2, 1 / fpr_dnet2, label='DenseNet 2')\n",
    "plt.plot(tpr_dnet, 1 / fpr_dnet, label='DenseNet 1')\n",
    "plt.plot(tpr_dnet0, 1 / fpr_dnet0, label='DenseNet 0')\n",
    "plt.plot(tpr_dnet_mean, 1 / fpr_dnet_mean, label='DenseNet Mean')\n",
    "# plt.plot(tpr_dnet_merged, 1 / fpr_dnet_merged, label='DenseNet')\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.98, 1))\n",
    "plt.xlabel('{} Efficiency'.format(CLASS_TWO))\n",
    "plt.ylabel('{} Background Rejection'.format(CLASS_ONE))\n",
    "plt.legend(fontsize=20, loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/mpaganini/venv/keras2tf1/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x8be72dd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJQCAYAAADolpLRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM2sC7tSZmZmZNQF36szMzMyawP8D1q4Clfj7oYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x888dff10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_dnet0, 1 / fpr_dnet0, label='DenseNet 0 no BN')\n",
    "plt.plot(tpr_dnet2, 1 / fpr_dnet2, label='DenseNet 2 no BN')\n",
    "plt.plot(tpr_dnet_merged, 1 / fpr_dnet_merged, label='DenseNet')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.98, 1))\n",
    "plt.xlabel('{} Efficiency'.format(CLASS_TWO))\n",
    "plt.ylabel('{} Background Rejection'.format(CLASS_ONE))\n",
    "plt.legend(fontsize=20, loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(tpr_image_dnn, fpr_image_dnn, label='Deep NN on Calorimeter Hits')\n",
    "plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='FCNN on Calorimeter Hits')\n",
    "plt.plot(tpr_feature_dnn, fpr_feature_dnn, label='DNN on Shower Shapes')\n",
    "plt.plot(tpr_feature_bdt, fpr_feature_bdt, label='BDT on Shower Shapes')\n",
    "# plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0.95, 1))\n",
    "plt.xlabel('{} Efficiency'.format(CLASS_ONE))\n",
    "plt.ylabel('{} Background Efficienct'.format(CLASS_TWO))\n",
    "plt.legend(fontsize=20, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from matplotlib import gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uix(a):\n",
    "    return np.unique(a, return_index=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eff_interp = np.linspace(0.05, 1, 100)\n",
    "# interp_image_dnn = np.interp(eff_interp, tpr_image_dnn, fpr_image_dnn)\n",
    "# interp_feature_dnn = np.interp(eff_interp, tpr_feature_dnn, fpr_feature_dnn)\n",
    "# interp_raveled_dnn = np.interp(eff_interp, tpr_raveled_dnn, fpr_raveled_dnn)\n",
    "\n",
    "interp_image_dnn = interpolate.interp1d(tpr_image_dnn[uix(tpr_image_dnn)], fpr_image_dnn[uix(tpr_image_dnn)], kind='cubic')(eff_interp)\n",
    "interp_feature_dnn = interpolate.interp1d(tpr_feature_dnn[uix(tpr_feature_dnn)], fpr_feature_dnn[uix(tpr_feature_dnn)], kind='cubic')(eff_interp)\n",
    "# interp_raveled_dnn = interpolate.interp1d(tpr_raveled_dnn[uix(tpr_raveled_dnn)], fpr_raveled_dnn[uix(tpr_raveled_dnn)], kind='cubic')(eff_interp)\n",
    "\n",
    "kern_size = 20\n",
    "kern = [1 / float(kern_size)] * kern_size\n",
    "interp_image_dnn = np.convolve(interp_image_dnn, kern, mode='same')\n",
    "interp_feature_dnn = np.convolve(interp_feature_dnn, kern, mode='same')\n",
    "# interp_raveled_dnn = np.convolve(interp_raveled_dnn, kern, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 2]) \n",
    "ax = plt.subplot(gs[0])\n",
    "plt.plot(tpr_image_dnn, fpr_image_dnn, label='ResNet on Calorimeter Hits')\n",
    "plt.plot(tpr_feature_dnn, fpr_feature_dnn, label='DNN on Shower Shapes')\n",
    "# plt.plot(tpr_raveled_dnn, fpr_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "plt.xlim((0, 1))\n",
    "plt.xlabel(r'$\\pi^{+}$ efficiency')\n",
    "plt.ylabel(r'$e^{+}$ efficiency')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(gs[1], sharex=ax)\n",
    "plt.plot(eff_interp, interp_feature_dnn / interp_image_dnn, label='ResNet on Calorimeter Hits')\n",
    "plt.plot(eff_interp, interp_feature_dnn / interp_feature_dnn, label='DNN on Shower Shapes')\n",
    "# plt.plot(eff_interp, interp_feature_dnn / interp_raveled_dnn, label='Simple DNN on Calorimeter Hits')\n",
    "# plt.yscale('log')\n",
    "plt.grid('on', 'both')\n",
    "# plt.xlim((0.99, 1))\n",
    "# plt.xlabel(r'$\\pi^{+}$ efficiency')\n",
    "plt.ylabel(r'Ratio of $e^{+}$ efficiencies of DNN on Shower Shapes to X')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
